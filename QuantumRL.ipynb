{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NV8sEgIgZisp",
        "hnWqxby5ZfY_",
        "kfmGXb2TZWpw"
      ],
      "authorship_tag": "ABX9TyMsItlCVTAJf+jWPz7Uw8Kl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNsunVDOmIM2"
      },
      "source": [
        "# Google QuantumAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDU__H-6mNf9"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQgeiVZmK4_",
        "outputId": "93d47049-425d-495c-ea9a-f8b91cba2e1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    import cirq\n",
        "except ImportError:\n",
        "    print(\"installing cirq...\")\n",
        "    !pip install --quiet cirq\n",
        "    print(\"installed cirq.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing cirq...\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 13.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 68.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 68.5MB/s \n",
            "\u001b[?25hinstalled cirq.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ05rJZRmPcL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng8kY2AmmXQS"
      },
      "source": [
        "import cirq\n",
        "from cirq import Simulator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano frtemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._keys = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvWLKKRgZR9"
      },
      "source": [
        "#### Google Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4R9ebGj2rl"
      },
      "source": [
        "class GoogleGates: \n",
        "  def __init__(self, keys):\n",
        "    PI = math.pi\n",
        "    self._keys = keys\n",
        "    self._gates = [\n",
        "                  (cirq.CNOT, None),\n",
        "                  (cirq.rx(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI)),\n",
        "                  (cirq.rx(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(2*PI/3)),\n",
        "                  (cirq.rx(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/2)),\n",
        "                  (cirq.rx(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/3)),\n",
        "                  (cirq.rx(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/4)),\n",
        "                  (cirq.ry(PI), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI)),\n",
        "                  (cirq.ry(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(2*PI/3)),\n",
        "                  (cirq.ry(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/2)),\n",
        "                  (cirq.ry(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/3)),\n",
        "                  (cirq.ry(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/4)),\n",
        "                  (cirq.rz(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI)),\n",
        "                  (cirq.rz(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(2*PI/3)),\n",
        "                  (cirq.rz(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/2)),\n",
        "                  (cirq.rz(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/3)),\n",
        "                  (cirq.rz(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/4))\n",
        "    ]\n",
        "\n",
        "    assert len(self._gates) == len(self._keys), f\"Expected number of keys equals ot number of gates, instead got {(len(self._gates), len(self._keys))}\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._keys, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._keys (Dict[String: Int]): dicitonary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._keys, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._state = next_state\n",
        "\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWp0UZ7q6VVF"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "  \n",
        "  def append_transition(self, transition):\n",
        "    self.buffer.append(transition)\n",
        "\n",
        "  def sample_transition(self):\n",
        "    return random.choice(self.buffer)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, num_offline_updates):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._state = initial_state\n",
        "    self._action = 17\n",
        "\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._replay_buffer = ReplayBuffer() # supervised dataset\n",
        "\n",
        "  def behaviour_policy(self, state = None):\n",
        "    # greedy = np.random.choice([True, False], p=[1-self._eps, self._eps])\n",
        "    # if greedy:\n",
        "    #   return np.argmax(self.q(state))\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    # Experience replay\n",
        "    self._replay_buffer.append_transition((s, a, r, g, next_s))\n",
        "    for _ in range(self._num_offline_updates):\n",
        "      s, a, r, g, next_s = self._replay_buffer.sample_transition()\n",
        "      self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy()\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade and Helper Functions\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good? ==> in this sense no, but this is because of local minima. I do not have guaranteee to find a global otimum with gradient descent (verify)\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, num_offline_updates, inference_ratio = 200):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._keys)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, num_offline_updates)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "    self._successes = 0\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    inference_terminal_states = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      gates, terminal_state = self.run_inference(episode)\n",
        "      inference_gates.append(gates)\n",
        "      inference_terminal_states.append(terminal_state)\n",
        "\n",
        "    return inference_gates, inference_terminal_states\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "\n",
        "  def run_inference(self, episode):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "      if not terminal:\n",
        "        gates.append(action)\n",
        "\n",
        "    max_gates = 20\n",
        "    if len(gates) < max_gates:\n",
        "      self._successes += 1\n",
        "      print(f\"Episode {episode}: inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates, self._env._terminal_state\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwKia38JvwL"
      },
      "source": [
        "def compare_circuits(circuit_gates, final_state, target_state):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    circuit_gates (List[int]): indices of the gates in the circuit defined by the agent\n",
        "    final_state (QuantumState): terminal state reached by the agent\n",
        "    target_state (QuantumState): target state of TD agent\n",
        "  \"\"\"\n",
        "  my_gates = Gates()\n",
        "  google_gates = GoogleGates(my_gates._keys)\n",
        "\n",
        "  # Google initialization\n",
        "  q0 = cirq.NamedQubit('q0')\n",
        "  q1 = cirq.NamedQubit('q1')\n",
        "  circuit = cirq.Circuit()\n",
        "  def basic_circuit(u0, u1):\n",
        "    if (u1 is not None):\n",
        "      yield u0(q0), u1(q1)\n",
        "    else:\n",
        "      yield u0(q0, q1)\n",
        "\n",
        "  q = QuantumState(np.array([1, 0, 0, 0]))\n",
        "  for idx in circuit_gates:\n",
        "    q = q.apply_gate(my_gates._gates[idx])\n",
        "    circuit.append(basic_circuit(*google_gates._gates[idx]))\n",
        "\n",
        "  simulator = Simulator()\n",
        "  result = simulator.simulate(circuit, qubit_order=[q0, q1])\n",
        "  print(f\"My implementation: {np.around(q.get_amplitudes(), 3)}\")\n",
        "  print(f\"Google QuantumAI: {np.around(result.final_state_vector, 3)}\")\n",
        "  print(f\"Difference: {np.around(result.final_state_vector - q.get_amplitudes(), 3)}\")\n",
        "  assert np.any(final_state.get_amplitudes() == q.get_amplitudes()),\\\n",
        "   f\"\"\"Unexpected termnal state: got {np.around(q.get_amplitudes(), 3)} rather than {np.around(final_state.get_amplitudes(), 3)}\"\"\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.1\n",
        "TRAINING_EPISODES = 100\n",
        "STEP_SIZE = 0.0001\n",
        "NUM_OFFLINE_UPDATES = 0 # Better remove it, I guess, we'll see"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "98a8b142-9fd0-47f6-df92-6a1013b434cd"
      },
      "source": [
        "%%time\n",
        "start = [1, 0, 0, 0]\n",
        "target = [0.5, 0.5, 0.5, 0.5]\n",
        "experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE, NUM_OFFLINE_UPDATES)\n",
        "gates_sequences, terminal_states = experiment.run_experiment()\n",
        "len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "print()\n",
        "print(experiment._successes)\n",
        "sns.heatmap(experiment._agent._W)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 9: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 18: inference completed in 16 steps. Fidelity score: 0.9536648003523602\n",
            "Episode 20: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 27: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 29: inference completed in 3 steps. Fidelity score: 1.0\n",
            "Episode 31: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 33: inference completed in 3 steps. Fidelity score: 0.9330127018922192\n",
            "Episode 37: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 38: inference completed in 2 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 42: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 43: inference completed in 4 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 44: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 46: inference completed in 7 steps. Fidelity score: 0.9330127018922196\n",
            "Episode 54: inference completed in 13 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 56: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 60: inference completed in 4 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 61: inference completed in 4 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 62: inference completed in 3 steps. Fidelity score: 1.0\n",
            "Episode 69: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 70: inference completed in 4 steps. Fidelity score: 0.9829629131445341\n",
            "Episode 71: inference completed in 4 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 72: inference completed in 3 steps. Fidelity score: 0.9330127018922194\n",
            "Episode 90: inference completed in 2 steps. Fidelity score: 0.9330127018922194\n",
            "\n",
            "23\n",
            "CPU times: user 37.2 s, sys: 3.93 s, total: 41.1 s\n",
            "Wall time: 35.9 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD5CAYAAAADQw/9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcVZn/8c+X3AFJSBCMCZIAQQTdBRmDeFmBEC4/WIK7sET2t2QVzHoBcd0VUHa5LbDgDfm56m5+XIyoXAwIQQIxEFBwuSQg93AJkSwTIEiCwSAhzMyzf1QNdoaZqe7p6q7unu+bV71SXX2q65khOXPmqVPPUURgZmaNb7OiAzAzs/K4wzYzaxLusM3MmoQ7bDOzJuEO28ysSbjDNjNrEkNrfYHff2K/wucNXverCUWHAMDR57yz6BBY+e+PFB0CAOP3fr3oEDjrjrcXHQIAp3/4xaJD4JbF44sOAYAjn/+xqv2MN15aUXafM2ybHau+Xj1ldtiSdgVmAN293ipgfkQsq2VgZma2qX5TIpJOAa4EBNybbgKukHRq7cMzM6tQV2f5W5PJGmEfB+weEW+UHpT0LeBR4PxaBWZmNiCdHUVHUDNZNx27gN4Sr+PT93olabakpZKW/mD5c9XEZ2ZWkYiusrdmkzXC/iJwq6SngGfTY+8CdgZO6OukiJgDzIHGuOloZoNIV/N1xOXqt8OOiJsl7QJMZdObjksiovkSQGbW+ppw5FyuzFkikfzecPdAL6CRQwZ6am4+ts3qokMA4OdfLf6XjfnDRxYdAgBtvxxbdAhc9YffFB0CADvcsWfRIcCwogPIURPeTCxXzedhm5nV1WAeYZuZNZMYxLNEzMyaS1dX+VsGSQdLekLS8t6ePZE0QtJV6fv3SJrU4/13SVov6Z/z+NLcYZtZa4mu8rd+SBoCfBc4BNgN+ISk3Xo0Ow54OSJ2Bi4ELujx/reAm3L5unCHbWatJr8nHacCyyNiRURsJHnqe0aPNjOAuen+PGCaJAFIOgL4LclDhrlwh21mrSWnETbJVOZnS16386fpzW9pExEdwDpgnKQtgVOAs3L5mlI1v+m48rZRtb5Eph0/MbzoEAC444rib4ZsTvHTLAGObWsvOgQm/frPiw4BgLs3K35Ww10dxVcMBDgpjw+p4KajpNnA7JJDc9IH/6p1JnBhRKxPB9y58CwRM2stFTzpWPpUdi9WAduXvJ6YHuutTbukocBoYA2wN3CkpK8BY4AuSRsi4j/KDq4X5ZZXnQDcExHrS44fHBE3V3NxM7O85fgQ9hJgiqTJJB3zTOCYHm3mA7OAu4AjgcUREcBHuxtIOhNYX21nDdnlVb8AXA+cCDwiqTThfl61Fzczy11OOew0J30CsBBYBlwdEY9KOlvS4WmzS0hy1suBLwE1LTudNcL+NLBXmoeZBMyTNCkiLiKpi92r0rzQv459H0e+bYecwjUzy5Bj8aeIWAAs6HHs9JL9DcBRGZ9xZl7xZHXYm3WnQSLiGUn7knTaO9BPh12aF3po0l8WX0DDzAaPFn40PWta32pJe3S/SDvvw4BtgPfVMjAzswHpfKP8rclkjbCPBTaZI5PmdY6V9F/lXGDMNn8cYGj5eegHjTGtb7ehxZdE+/jOz2Y3qoP3Ln6p6BA4dnTxiyIDnPyB4hf5eOGBzYsOIT+DuB52n5NlI+LX+YdjZlalFk6JeB62mbWWwTrCNjNrOu6wzcyaQzThzcRyucM2s9biHLaZWZNwSmTgnnp2m1pfIjuG4cVPpwNYNqT4an3bLutZHbIY+2w1uugQmP5a8f8/AP72nuKn1F2x3+tFh5Afj7DNzJpEC4+wK17AQNIPaxGImVku8lvAoOH0O8KWNL/nIWA/SWMAIuLwt55lZlagjsZIddVCVkpkIvAYcDEQJB12G/DN/k4qrdb3xbftxWGjdqo+UjOzcjThyLlcWSmRNuA+4DRgXUTcDrwWEb+MiF/2dVJEzImItohoc2dtZnXV1VX+1mSyaol0ARdK+mn65+qsc8zMCtXCI+yyOt+0CNRRkg4FXqnkAtuPXTeQuHJ14auNsfDsv2wcUXQI3D6i+ClkAJfsX3wVx+d/3Ri5zqmvb110CAyd3EKdXBOOnMtV0Wg5Im4EbqxRLGZm1RvsI2wzs6bRwrNEKp6HbWbW0CLK3zJIOljSE5KWS3rLAruSRki6Kn3/nnTtWyRNlfRAuj0o6eN5fGkeYZtZa8kphy1pCPBdYDrQDiyRND8iHitpdhzwckTsLGkmcAFwNPAI0BYRHZLGAw9KuiFdsWvAPMI2s9aS37S+qcDyiFgRERuBK4EZPdrMAOam+/OAaZIUEX8s6ZxHkjzHUjV32GbWWip4NF3SbElLS7bZJZ80AShdBLU9PUZvbdIOeh0wDkDS3pIeBR4GPlPt6BrqkBKZ+Onta32JTG0XFT+FDODdH15VdAjc2QCV4QBeWlL8VMuTXstl0FO1K9r6XDq1bq68uPh/pwDHnZnDh3R2lt00IuYAc3K4am+ffQ+wu6T3AHMl3RQRG6r5TI+wzay15JcSWQWU/iSbmB7rtY2kocBoYE1pg4hYBqwH3lvFVwVkdNjpkH6rdH+UpLMk3SDpAknFFzQ2M+spvw57CTBF0mRJw4GZQM+CePOBWen+kcDiiIj0nKEAknYAdgWeqfZLyxphXwp05xMuIvnpcUF67LJqL25mlrucyqumOecTgIXAMuDqiHhU0tmSuiuVXgKMk7Qc+BLQPfXvIyQzQx4AfgZ8LiJeqvZLy8phb1aSKG+LiPen+3emgfSqtFrfd478CJ/aZ9dq4zQzK0t05XdvIiIWAAt6HDu9ZH8DcFQv510OXJ5bIKmsEfYjkj6Z7j8oqQ1A0i5An0sTl1brc2dtZnU1WKv1AccDF0n6F+Al4C5Jz5JMYzm+1sGZmVWsglkizSarvOo64O/TG4+T0/btEbG63At0PvlsdqMaW8bIokMA4Bd3Fr8A7ngVHUHiyj8UvzjzbkMa4x/2HXcVX62vpZ55bsKRc7nKLa/6CvBgjWMxM6veYO+wzcyaRhlFnZqVO2wzay0eYZuZNYkcp/U1GnfYZtZaBussETOzZhNOiQzcNTcUP31r/waZytY+rOgI4OXNGuMvcyP80vpXGxpjKakrRhZfg+1DG4cXHUJ+nBIxM2sSg3UR3pIKVc9FxC2SjgE+RFIIZU5E9Pl4uplZIQbxCPuytM3mkmYBWwLXAtNIls+Z1c+5Zmb11zF4bzq+LyL+LK3rugp4Z0R0SvoR/Tz5WFqtb9boqey7xZTcAjYz61cLp0Sy7nZslqZF3gZsTlIPG2AE0OcttNJqfe6szayuuqL8rclkjbAvAR4HhgCnAT+VtAL4IMkKwmZmDWXQTuuLiAslXZXuPyfph8ABwP+PiHvLucD7h62rPsoqfZ9RRYcAwEnD1xcdAotfG1t0CABM2Vj8/eqfjip+IWCA8w/6fdEhMGz6R4sOIT9NOHIuV+a0voh4rmT/98C8mkZkZlaNFu6wi5+xb2aWp87O8rcMkg6W9ISk5ZJO7eX9EZKuSt+/R9Kk9Ph0SfdJejj9c/88vjQ/OGNmLSWvNR0lDQG+C0wH2oElkuZHxGMlzY4DXo6InSXNJFmk/GiSFbr+Mk0lv5dkId+qVzDxCNvMWkt+s0SmAssjYkVEbCSZaDGjR5sZwNx0fx4wTZIi4jcl6eRHgVGSRlT7pbnDNrPWkt8ivBNI1q/t1s5bR8lvtomIDmAdMK5Hm78G7o+I1wf8NaWcEjGz1lJBSqT0Ib/UnIiYk1coknYnSZMcmMfn1bzD3unYLWp9iUwdP2yMR1U/uvrpokNg79GNUbrwfaNGZzeqsfveeKHoEAB4+PoGmGp5/QNFRwDAh4/M4UMq6LDTzrmvDnoVsH3J64npsd7atKdPhI8G1gBImgj8DDg2InL5x++UiJm1lOjsKnvLsASYImlySSG8+T3azOdPNZWOBBZHREgaA9wInBoRv87ra3OHbWatJaebjmlO+gSSGR7LgKsj4lFJZ0s6PG12CTBO0nLgS0D31L8TgJ2B0yU9kG7bVvulOYdtZi0lr2l9ABGxAFjQ49jpJfsbgKN6Oe8c4JzcAkn1O8KWNFrS+ZIel7RW0hpJy9JjY/o5b7akpZKWXnp/8XlbMxtEWrj4U1ZK5GrgZWDfiBgbEeOA/dJjV/d1Umm1vk+9f6f8ojUzy9JVwdZkslIikyLigtIDEfECcIGkT9UuLDOzgYmOJuyJy5TVYa+UdDIwNyJWA0jaDvh7Np1Q3qezf1j8NLIvj/pD0SEA8FrsXnQIHPpGY1Qu3CE2FB0Cjw3bvOgQAHi1o/hbSaPUGAsS56J1++vMlMjRJE/t/DLNYa8FbgfG0kui3cysaNEVZW/NJqse9svAKem2CUmfJFnz0cyscQziEXZ/zsotCjOznAzaEbakh/p6C9gu/3DMzKrUwiPsrLsd2wEHkUzjKyXgv2sSkZlZFaKF7p/2lNVh/xzYMiLeUhlG0u01icjMrAoxWEfYEXFcP+8dU84FPjvmpUpjyt3G1xtjsdVD3ih+GtnKYUVHkHgPxS/CuyfFVwwE2Ontxf8buWvt24sOAYBclgIerB22mVmzGbQjbDOzZuMO28ysSURn8U9X10pWtb6tJP27pMslHdPjve/1c96b1fp+sqY9r1jNzDJFV/lbs8l6cOYykil81wAzJV1TsvLvB/s6qbRa3zHjJuYUqplZtuhS2VuzyUqJ7BQRf53uXyfpNGBxyWoLZmYNpRlHzuXK6rBHSNosIvkWRMS5klYBvwK2LOcC2/3jXlWGWL1XLru76BAAGPH74h+FfUdHY4wqOlT86nTjaYzvRSNMO13TQnezIhrj/2stZP2ruQHYv/RARPwA+CdgY41iMjMbsEGbw46IkyPill6O3wycV7OozMwGqKtTZW9ZJB0s6QlJyyWd2sv7IyRdlb5/j6RJ6fFxkm6TtF7Sf+T1tblan5m1lLxuOkoaAnwXOATYDfiEpN16NDsOeDkidgYuBLpX6NoA/Cvwz3l+ba7WZ2YtJcfZH1OB5RGxAkDSlcAM4LGSNjOAM9P9ecB/SFJEvArcKWnnvIIBV+szsxYT+d3bn8CmSyG2A3v31SYiOiStI1mlqyYFYlytz8xaSiUjbEmzgdklh+ZExJzcg8pJzav1PX3WI5XGlLsrut5ZdAgA/N3bev6iUn9Pr2uMCnX7r+8r21Y/87acWnQIAPzbkBHZjWrs21PKWlO7KVQyrS/tnPvqoFcB25e8npge661Nu6ShwGhgTdkBVKj4ybBmZjnq7FTZW4YlwBRJkyUNB2YC83u0mQ/MSvePBBZH5JiU6aGFpsubmeX34Eyakz4BWAgMAS6NiEclnQ0sjYj5wCXA5ZKWA2tJOnUAJD0DbAUMl3QEcGBEPNbzOpWouMOWtG1EvFjNRc3MaiXPGiERsQBY0OPY6SX7G4Cj+jh3Um6BpLKq9Y3tsY0D7pW0taSx/Zz3ZrW+n77yP3nHbGbWp4jyt2aTNcJ+CVjZ49gE4H4ggB17O6k0kf/Ijoc14bfFzJpVM1bhK1dWh/1lYDrw5Yh4GEDSbyNics0jMzMbgM6u1p1LkTWt75uSrgIulPQscAbJyLps75jyhyrCy8cHH3xb0SEAMJcxRYfAEUM2FB0CAJ99e8/nD+rvpD8+XXQIANx34p8VHQKb7Tcru1GTaMZUR7kybzpGRDtwVFoDexFQ/NLfZmZ96BrE5VXflE5h2Q84AEDSJ2sVlJnZQEWo7K3ZVJTsiYjXIqL70UVX6zOzhjNoZ4m4Wp+ZNZtWTom4Wp+ZtZRBO0sEV+szsybThJmOstW8Wt9vHnhHpTHl7tGRxS9yCrAyXi06BO4cukXRIQDwiw2/LToETh62S9EhADDs779adAj8cI/TsxvVwXHtR1T9GYM5JWJm1lSacfZHudxhm1lLacLF0MtWcXY+LQBlZtaQApW9NZusan3nS9om3W+TtAK4R9JKSR/r57w3q/Xd+FpjPP5rZoNDR6jsrdlkjbAPjYjuxSS/DhydLuc+HfhmXydFxJyIaIuItkNH7ZRTqGZm2Vp5hJ2Vwx4qaWhEdACjImIJQEQ8Kan4hejMzHpo5Rx2Vof9PWCBpPOBmyVdBFwL7A+8ZW52b0aqs7oIczAshhUdAgBHbxxVdAjcPrKj6BAAuGmX4v+frH/pd0WHAMBzB87OblRjh+7YGH8v8tCMI+dy9ZsSiYjvAOcB/wDMIOmoTyFZKdjFn8ys4XRVsGWRdLCkJyQtl3RqL++PkHRV+v49kiaVvPeV9PgTkg6q+gujvPKqtwO39xLoJ4HL8gjCzCwvnTmNsCUNAb5Lcs+uHVgiaX6PhXSPA16OiJ0lzQQuAI6WtBvJgry7A+8EbpG0S0RUlXKo5qF7V+szs4bTpfK3DFOB5RGxIiI2AleSZBpKzQDmpvvzgGmSlB6/MiJej4jfAsvTz6uKq/WZWUvpyi+HPQF4tuR1O9BzqaQ320REh6R1wLj0+N09zp1QbUCu1mdmLaWS4k+SZgOld33npIuINyRX6zOzllLJtL60c+6rg14FbF/yemJ6rLc27ZKGAqOBNWWeW7GaV+tr+9EBlcaUu+HH/qLoEAB4/yPfKDoE3vPRzxUdAgBf/Z9tig6B2RsbY8bu4yp+mdSn1jbG9+LcHD6jS7mlRJYAUyRNJulsZwI9+735wCzgLuBIYHFEhKT5wE8kfYvkpuMU4N5qA3LxJzNrKXk9+ZHmpE8AFgJDgEsj4lFJZwNL03VuLwEul7QcWEvSqZO2uxp4DOgAPl/tDBFwh21mLaaM2R9li4gFwIIex04v2d8AHNXHueeSzy8Nb3KHbWYtJcdZIg0nq1pfm6TbJP1I0vaSFklaJ2mJpD37Oe/Nan2X3HhH/lGbmfUhKtiaTTm1RM4AxpBM4/vHiJguaVr63j69nVR65/W1W/6zGb8vZtak8kyJNJqsJx2HRcRNEXEFEBExj2TnVmBkzaMzM6tQnrVEGk3WCHuDpANJ5haGpCMi4rp08YKy7nje9beLq42xagtHNUYl2NP3/HzRIXBOV/EVAwEuPKDns1j1t2BR8QtEA2zf8UbRIbC0+OKJuels4RF2Vof9GeBrJD+MDgI+K+kHJHMSP13b0MzMKteMI+dyZZVXfTAiDoqIQyLi8Yg4KSLGRMTuwLvrFKOZWdlaOSXian1m1lJC5W/NxtX6zKylNOPIuVyu1mdmLaX4RQlrx9X6zKyltPI87JpX6xuq4p+b2SqqSdXn5+tDiq8E8J9DGmP+1iELty06BB4aWfzfTYD9Xiu+h9nrjeL/buZlMKdEzMyaijtsM7Mm0Ri/N9VGVvGn0ZLOl/S4pLWS1khalh4bU68gzczKleMivA0nK7l7NckMkX0jYmxEjAP2S49d3ddJpdX65v9xRX7Rmpll6KxgazZZHfakiLggIl7oPhARL0TEBcAOfZ0UEXMioi0i2g7ffMe8YjUzy9RFlL01m6wOe6WkkyW9+ZCMpO0kncKmy7+bmTWEVn40Peum49HAqcAv0047gNUkC0/+TTkXOLH6hYKrtvjPGmMq2w33b5/dqMa+fsKQokMAYOb31xQdAh+KxrgNc9Oo4ruOA15rxgRB75pv3Fy+rOJPLwOXAScA26d57PdExCnA1HoEaGZWiXqNsCWNTVfheir9c+s+2s1K2zwlaVbJ8XMlPStpfbnXzJol8gXgepIO+xFJM0rePq/ci5iZ1UuHouytSqcCt0bEFODW9PUmJI0lWbVrb5JB7hklHfsNVDjwzcphfxrYKyKOAPYF/lXSSd2xVHIhM7N6qOOajjOAuen+XOCIXtocBCyKiLVpxmIRcDBARNwdEc9XcsGsHPZmEbE+/fBnJO0LzJO0A+6wzawB1fGOwHYlHe4L9F7BdAKbTtBoT48NSFaHvVrSHt3FnyJivaTDgEuB9w30omZmtVLJdD1Js4HZJYfmpIuId79/C9DbWnKnlb6IiJBqXzgpq8M+FugoPRARHcCxkv6rZlGZmQ1QJb1m2jnP6ef9A/p6T9JqSeMj4nlJ44EXe2m2iiSd3G0icHsFIW4iq1pfez/v/bqcCzy6dmWlMeVu6Jhdiw4BgF8Pe73oELjwouKnWQJMHTng3wpz85kP9PnXu66+tGRc0SGwzwdfKjqE3NQxJTIfmAWcn/55fS9tFgLnldxoPBD4ykAv2Bh1R83MctJJlL1V6XxguqSngAPS10hqk3QxQESsBf4NWJJuZ6fHkPQ1Se3A5pLaJZ2ZdUFX6zOzllKvEXZErAGm9XJ8KXB8yetLSe779Wx3MnByJdfMmoe9laR/l3S5pGN6vPe9Si5kZlYPUcF/zSYrJXIZyfS9a4CZkq6RNCJ974N9nVRara+r69WcQjUzyzaYa4nsFBF/ne5fJ+k0YLGkw/s7qfTO69DhE5rvx5iZNa1mrMJXrqwOe4SkzSKiCyAizpW0CvgVsGXNozMzq1DrdtfZHfYNwP7ALd0HIuIHkl4AvlPOBRZu/ZGBR5eT1597IbtRHZyy+StFh8AFA3/IKlf7bxxedAh8c8k7iw4BgG0boIs5/oHRRYcAwLU5fEZHA3w/ayWrWt/JQLukaZK2LDl+M/CFWgdnZlapQXvTUdKJJJPBT+St1frOrWVgZmYDMZhvOs4mqda3XtIkksJPkyLiIlz8ycwaUDOOnMvlan1m1lKaceRcrqx52Ksl7dH9Iu28DwO2wdX6zKwBdUaUvTUbV+szs5YyaOdh51Gt77KRb1QaU+7mHP6eokMAYN43yl66rWb+adTaokMAYOJfFV+58KFrG+NRgg+8XnwNtr9rgIWA8zKYc9hmZk2ldX70vFXFHbakbSOit0LdZmaFG7QpkXTF300OAfdK2hNQd11XM7NGMZhTIi8BPZeMmQDcT/LI/o69nVS6TtrUsXswZcvJVYZpZlaeZpz9Ua6sux1fBp4ADo+IyRExGWhP93vtrCGp1hcRbRHR5s7azOqpiyh7azZZs0S+Kekq4EJJzwJn0NrFsMysyQ3qm47p1L6j0hrYi4DNK7nAyo51AwwtPydfOKzoEAD43PDip/Xt89yTRYcAwCN3vavoEDjjx0cXHQIAt/7Vz4sOoaW0cg47cwKopF0lTQMWA/uRLDaJpINrHJuZWcXqlRKRNFbSIklPpX9u3Ue7WWmbpyTNSo9tLulGSY9LelTS+eVcM6ta3xcoqdYHHBgRj6Rvn1f2V2ZmVicRUfZWpVOBWyNiCnBr+noT6Uy7M4C9ganAGSUd+zciYldgT+DDkg7JumBWSuTTuFqfmTWRzvqlRGYA+6b7c4HbgVN6tDkIWNQ9BVrSIuDgiLgCuA0gIjZKuh+YmHVBV+szs5ZSSaqjdApyak66Jm05touI59P9F4DtemkzAXi25HV7eqw0hjHAXwIXZV0wq8NeLWmPiHgAkmp9kg4DLsXV+sysAVWS6ihdMLw3km4B3tHLW6f1+JyQVPHQXtJQ4Arg/0XEiqz2rtZnZi0lz/nVEXFAX+9JWi1pfEQ8L2k80FvJjlX8KW0CSdrj9pLXc4CnIuLb5cRT82p9Px5fTqvaGvfxEUWHAMCan71UdAhc9urUokMAYOwX3110CHz7mJuKDgGAMcOHFB0CH/noa0WHkJs6TuubD8wCzk//vL6XNguB80puNB4IfAVA0jnAaOD4ci9YfF1HM7Mc1XEBg/OB6ZKeIpnufD6ApDZJFwOkNxv/DViSbmdHxFpJE0nSKrsB90t6QFJmxz2Qan3jImJNpeeZmdVDvR45T/vBab0cX0rJqDkiLiW571fapp0BTNzImod9vqRt0v02SSuAeyStlPSxSi9mZlZrrVxLJCslcmhEdCdevw4cHRE7A9OBb/Z1kqTZkpZKWvqT363KKVQzs2x1fHCm7rJSIkMlDU1nhoyKiCUAEfGkpD7v5JVOlfmftmnN910xs6bVjCPncmV12N8DFqTPud8s6SLgWmB/4IFaB2dmVqlWLv6UNa3vO5IeBj4L7JK2nwJcB5xTzgW2/f7nqo2xauu+/J2iQwBg9PuKX0Lzyd81xsSgu055vOgQGEPx0+mgx4MOBXloYa91i+ruozl8Rme0boHVcnqQF0jSG/d0P6YOb1bru7lWgZmZDUQz5qbLVVG1PkkzSt52tT4zazitPEvE1frMrKUM2hw2rtZnZk2ma7CmREir9XW/SDvvw4BtcLU+M2tAUcF/zcbV+syspQzaWSJ5VOuL5Q9VGlPuTn1mXNEhALDut28UHQKXfqgxnjz9z/+ekN2oxr7x8r1FhwDAyVsXX0FxzojG6OTymNbXyimR4icGm5nlqBlTHeVyh21mLaWVR9hZ87DbJN0m6UeStk+Xcl8naYmkPesVpJlZuVr5pmPWLJHvAV8DbgT+G/iviBhNspz79/o6qbRa3yW33pdbsGZmWTqjs+yt2WR12MMi4qZ0SfaIiHkkO7cCI/s6KSLmRERbRLQdN22vHMM1M+vfYC6vukHSgSTrjoWkIyLiunTxgub78WRmLa8ZHzkvV1aH/RmSlEgXcBDwWUk/IFkJ+NPlXOC6f1peTXy5eITfFR0CAMcMfVfRIXD5nVsVHQIAe2wsvkbdflu/p+gQADjxO3tkN6qx3/zDXUWHkJt6jZwljQWuAiYBzwB/ExEv99JuFvAv6ctzImJuevxmYDxJP3wH8PmI/vM0/aZEIuJB4IvAN4D2iDgpIsZExO5AY/zLNzMr0RVR9lalU4FbI2IKcGv6ehNpp34GsDcwFTijZAX1v4mIPwfeC7wdOCrrguVU6/sZrtZnZk2ijrNEZgBz0/25wBG9tDkIWBQRa9PR9yLgYICIeCVtMxQYDtkBlVOtr83V+sysWdTx0fTtIuL5dP8FYLte2kwAni153Z4eA0DSQpKR903AvKwLulqfmbWUSnLYkmYDs0sOzUnXpO1+/xbgHb2celqPa4akiofsEXGQpJHAj0mWXlzUX/usDnu1pD0i4oH0w9dLOgy4FFfrM7MGVEluunTB8D7eP6Cv9yStljQ+Ip6XNB54sZdmq4B9S15PBG7vcY0Nkq4nSbH022FnzcM+lmSoX/rhHRFxLPAXGeeamcTvbSIAAAVbSURBVNVdHedhzwdmpfuzSFbn6mkhcKCkrdObjQcCCyVtmXbySBoKHApkLnSqWk+BWbTd0YVPitxis+KnkAHcNWxU0SFwS6wpOgQAju0cW3QI7Kw/Fh0CAA9pi6JD4PYhjfG9uHzltVWnWkdvuVPZfc669U8P+HqSxgFXA+8CVpLM+lgrqQ34TEQcn7b7FPDV9LRzI+IySdsBPwdGkAycbwP+MS1f3ScXfzKzllKvedgRsQaY1svxpcDxJa8vJUkjl7ZZDXyg0mu6wzazljJoFzBIcyvHAR8H3pkeXkWSq7kkIoqvyG9mVmLQllcFLgf2AM4E/k+6nQX8OfCjvk4qrdZ342tP5xSqmVm2wVz8aa+I2KXHsXbgbklP9nVS6VSZRrjpaGaDRzPWuS5X1gh7raSjJL3ZTtJmko4G3lLkxMysaIN5hD0TuAD4rqTfp8fGkExBmVnLwMzMBqKVc9iZ87Al7U1SlORpYFdgH+CxiFhQ+/DejGF26eOiRWiEGBoljkaIoVHiaIQYGiWORoih1fXbYUs6AziEZCS+iKRIye3AdGBhRJxbhxiRtDQi2upxrUaOoVHiaIQYGiWORoihUeJohBhaXVZK5EiSWSIjSB5RnxgRr0j6BnAPUJcO28zMsm86dkREZ0T8EXi6u35rRLxGsgqNmZnVSVaHvVHS5un+m6vpShpNfTvsRsiLNUIM0BhxNEIM0BhxNEIM0BhxNEIMLS0rhz0iIl7v5fg2wPiIeLiWwZmZ2Z/UvFqfmZnlIyslUihJB0t6QtJySW9Z4LJOMVwq6UVJjxRx/TSG7SXdJukxSY9KOqmgOEZKulfSg2kcZxURRxrLEEm/kfTzAmN4RtLDkh6QtLSgGMZImifpcUnLJO1TQAzvTr8H3dsrkr5Y7zgGg4YdYUsaAjxJMoWwHVgCfCIiHqtzHH8BrAd+GBHvree1S2IYT5KCul/S24D7gCMK+F4I2CJdeWgYcCdwUkTcXc840li+BLQBW0XEYfW+fhrDMyRrnr5UxPXTGOYCd0TExZKGA5tHxO+zzqthPENICsTtHREri4qjVTXyCHsqsDwiVkTERuBKkiV06ioifgWsrfd1e8TwfETcn+7/AVhGyUKedYwjutf4BIalW91/4kuaSLJCx8X1vnYjSW/+/wVwCUBEbCyys05NI5lR5s66Bhq5w+53teHBKl29fk+SefBFXH+IpAdI1q9bFBFFxPFt4GSKn1oawC8k3Zcu5lpvk4HfAZel6aGLpcKXr5kJXFFwDC2rkTts60HSlsA1wBe758TXWzovfw+SxUSnSqprmihdBPrFiLivntftw0ci4v0kTwN/Pk2f1dNQ4P3A9yNiT+BVoJB7PQBpSuZw4KdFxdDqGrnDXgVsX/J6YnpsUEpzxtcAP46Ia4uOJ/3V+zbg4Dpf+sPA4Wn++Epgf0l91mavpYhYlf75IvAzkjRePbUD7SW/5cwj6cCLcghwf7r8ldVAI3fYS4ApkianP7lnkqxSPOikN/suAZZFxLcKjOPtksak+6NIbghnrvScp4j4SkRMjIhJJH8nFkfE/61nDACStkhvAJOmIQ4E6jqTKCJeAJ6V9O700DSgrjeie/gETofUVMOu6RgRHZJOIFkmfghwaUQ8Wu84JF0B7AtsI6kdOCMiLqlzGB8G/g54OM0fA3y1nhUTU+OBuelMgM2AqyOisGl1BdsO+Fnys5ShwE8i4uYC4jgR+HE6qFkBfLKAGLp/aE0H/qGI6w8WDTutz8zMNtXIKREzMyvhDtvMrEm4wzYzaxLusM3MmoQ7bDOzJuEO28ysSbjDNjNrEu6wzcyaxP8CI+S/DlKLKTsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86va33c3J3X_",
        "outputId": "7c13509b-a6e8-4bbf-f916-f4886ae1fa12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "idx = np.argmin(list(map(lambda x: len(x), gates_sequences)))\n",
        "circuit_gates = gates_sequences[idx]\n",
        "terminal_state = terminal_states[idx]\n",
        "target_state = QuantumState(target)\n",
        "\n",
        "print(target_state.fidelity_score(terminal_state))\n",
        "\n",
        "compare_circuits(circuit_gates, terminal_state, target_state)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9330127018922194\n",
            "My implementation: [0.612 0.354 0.612 0.354]\n",
            "Google QuantumAI: [0.612+0.j 0.354+0.j 0.612+0.j 0.354+0.j]\n",
            "Difference: [-0.+0.j -0.+0.j -0.+0.j -0.+0.j]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWiTttQxLzUg",
        "outputId": "511d3f32-adf7-4d6c-986b-607bb09910b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mapping = experiment._env._keys\n",
        "for el in circuit_gates:\n",
        "  print(mapping[el])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ry0(pi/2)\n",
            "Ry1(pi/3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoEfGy_AxDmb"
      },
      "source": [
        "# Analysis and TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-DR4p_QJ8o"
      },
      "source": [
        "Count the number of successes.\n",
        "Some summary statistics of this type using binning should be plotted\n",
        "In general this could be useful to compare different polcies\n",
        "Please build a systematic set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6yyJGq1J9Oh"
      },
      "source": [
        "Compare \n",
        "- Step size decrease (che e un concettoesistente in Deep lr ==> scheduler): è migliorativo?\n",
        "- behavioural policy: random o eps greedy?\n",
        "- Different rewards\n",
        "- Ragionare su perché il miglioramente non è costante, come invece vorrei,\n",
        "==> ho bisogno di generalizzare? no devo essere di successo solo su cio che ho visto in training\n",
        "Behaviour policy always exploring has been solution: try without.\n",
        "Nota: la ragione del comportamento oscillante da episodio a episodio è dovuto al fatto che gradient descent rischia di cadere in local optima ==> rallenta la convergenza a una policy ottimale. Quanto invece ho dei buoni seed esco di lì. \n",
        "Devo mediare o cosa? Posso anche mediare, ma dire che a noi serve un global optimum per trovare l'optimal path, non generalizzare. Chiedo a tizio.\n",
        "\n",
        "- Comparo diverse target policy da diversi behaviour!\n",
        "- testare experience replay per capire al meglio ogni stato. ==> mi serve o posso togliere?\n",
        "- Comparo numero di policy di successo rispetto a un approccio random: però la policy che imparo è imparata, il random no, cretino ==> quindi anche se e peggio in generale, basta sia buono una volta. Probabilmente il minimo che trovo è oslo locale, ma è gia sufficiente, e il fatto che ne esca continuamente richiede soluzione a piu ampio spettro a cui non sono interessato ora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7dwCD0J9Oi"
      },
      "source": [
        "Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ]
    }
  ]
}