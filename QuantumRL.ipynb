{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NV8sEgIgZisp",
        "hnWqxby5ZfY_",
        "kfmGXb2TZWpw"
      ],
      "authorship_tag": "ABX9TyN3NB05QVZHlekIrGNAVSGF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNsunVDOmIM2"
      },
      "source": [
        "# Google QuantumAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDU__H-6mNf9"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQgeiVZmK4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec4d6af-d890-4e0f-e516-0a80f15bfbe0"
      },
      "source": [
        "try:\n",
        "    import cirq\n",
        "except ImportError:\n",
        "    print(\"installing cirq...\")\n",
        "    !pip install --quiet cirq\n",
        "    print(\"installed cirq.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing cirq...\n",
            "\u001b[K     |████████████████████████████████| 389kB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 10.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 24.3MB/s \n",
            "\u001b[?25hinstalled cirq.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ05rJZRmPcL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng8kY2AmmXQS"
      },
      "source": [
        "import cirq\n",
        "from cirq import Simulator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano fortemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._keys = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvWLKKRgZR9"
      },
      "source": [
        "#### Google Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4R9ebGj2rl"
      },
      "source": [
        "class GoogleGates: \n",
        "  def __init__(self, keys = list()):\n",
        "    PI = math.pi\n",
        "    self._keys = keys\n",
        "    self._gates = [\n",
        "                  (cirq.CNOT, None),\n",
        "                  (cirq.rx(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI)),\n",
        "                  (cirq.rx(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(2*PI/3)),\n",
        "                  (cirq.rx(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/2)),\n",
        "                  (cirq.rx(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/3)),\n",
        "                  (cirq.rx(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/4)),\n",
        "                  (cirq.ry(PI), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI)),\n",
        "                  (cirq.ry(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(2*PI/3)),\n",
        "                  (cirq.ry(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/2)),\n",
        "                  (cirq.ry(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/3)),\n",
        "                  (cirq.ry(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/4)),\n",
        "                  (cirq.rz(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI)),\n",
        "                  (cirq.rz(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(2*PI/3)),\n",
        "                  (cirq.rz(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/2)),\n",
        "                  (cirq.rz(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/3)),\n",
        "                  (cirq.rz(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/4))\n",
        "    ]\n",
        "\n",
        "\n",
        "  def num_gates(self):\n",
        "    return len(self._gates)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "class Environment:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._keys, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._keys (Dict[String: Int]): dictionary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._keys, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if the terminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._state = next_state\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWp0UZ7q6VVF"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "  \n",
        "  def append_transition(self, transition):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      transition (tuple): (state, action, reward, discount, next state)\n",
        "    \"\"\"\n",
        "    self.buffer.append(transition)\n",
        "\n",
        "  def sample_transition(self):\n",
        "    return random.choice(self.buffer)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, num_offline_updates):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._initial_state = initial_state\n",
        "\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._replay_buffer = ReplayBuffer() # supervised dataset\n",
        "\n",
        "    # St self._state to initial state and pick an action randomly\n",
        "    self.reset(True)\n",
        "\n",
        "\n",
        "  def behaviour_policy(self, state = None):\n",
        "    # greedy = np.random.choice([True, False], p=[1-self._eps, self._eps])\n",
        "    # if greedy:\n",
        "    #   return np.argmax(self.q(state))\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    # Experience replay\n",
        "    self._replay_buffer.append_transition((s, a, r, g, next_s))\n",
        "    for _ in range(self._num_offline_updates):\n",
        "      s, a, r, g, next_s = self._replay_buffer.sample_transition()\n",
        "      self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy()\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "\n",
        "  def reset(self, random = False):\n",
        "    self._state = self._initial_state\n",
        "    if random:\n",
        "      self._action = self.behaviour_policy()\n",
        "    else:\n",
        "      self._action = np.argmax(self.q(self._state))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5_vQxz7Ke2s"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwKia38JvwL"
      },
      "source": [
        "def compare_circuits(circuit_gates, final_state, target_state):\n",
        "  \"\"\"\n",
        "  Compare results obtained by google gates vs my custom gates implementation\n",
        "  Args:\n",
        "    circuit_gates (List[int]): indices of the gates in the circuit defined by the agent\n",
        "    final_state (QuantumState): terminal state reached by the agent\n",
        "    target_state (QuantumState): target state of TD agent\n",
        "  \"\"\"\n",
        "  my_gates = Gates()\n",
        "  google_gates = GoogleGates(my_gates._keys)\n",
        "\n",
        "  # Google initialization\n",
        "  q0 = cirq.NamedQubit('q0')\n",
        "  q1 = cirq.NamedQubit('q1')\n",
        "  circuit = cirq.Circuit()\n",
        "  def basic_circuit(u0, u1):\n",
        "    if (u1 is not None):\n",
        "      yield u0(q0), u1(q1)\n",
        "    else:\n",
        "      yield u0(q0, q1)\n",
        "\n",
        "  # Create the circuit with both Google API and my implementation\n",
        "  q = QuantumState(np.array([1, 0, 0, 0]))\n",
        "  for idx in circuit_gates:\n",
        "    q = q.apply_gate(my_gates._gates[idx])\n",
        "    circuit.append(basic_circuit(*google_gates._gates[idx]))\n",
        "\n",
        "  # Simulate Google circuit - oracle\n",
        "  simulator = Simulator()\n",
        "  result = simulator.simulate(circuit, qubit_order=[q0, q1])\n",
        "\n",
        "  assert np.all(np.around(q.get_amplitudes(), 3) == np.around(final_state.get_amplitudes(), 3)),\\\n",
        "   f\"Unexpected terminal state: got {np.around(q.get_amplitudes(), 3)} rather than {np.around(final_state.get_amplitudes(), 3)}\"\n",
        "\n",
        "  print(f\"Custom gates approximate state: {np.around(final_state.get_amplitudes(), 3)}\")\n",
        "  print(f\"Google approximate state: {np.around(result.final_state_vector, 3)}\")\n",
        "  print(f\"Target state: {np.around(target_state.get_amplitudes(), 3)}\")\n",
        "  print(f\"Fidelity score: {target_state.fidelity_score(final_state)}\")\n",
        "\n",
        "\n",
        "  print(\"\")\n",
        "  print(circuit)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyWWqvKkQSMj"
      },
      "source": [
        "def get_quantum_state(tolerance, i):\n",
        "  \"\"\"\n",
        "  Get a feasible set of amplitudes of a quantum state, \n",
        "  by applying a random number of random gates to the initial state |00>\n",
        "\n",
        "  Return:\n",
        "    amplitudes (np.array): list of new state amplitudes\n",
        "  \"\"\"\n",
        "  random.seed(43 + i)\n",
        "  np.random.seed(43 + i)\n",
        "\n",
        "  gates = GoogleGates()\n",
        "\n",
        "  # Circuit initialization\n",
        "  q0 = cirq.NamedQubit('q0')\n",
        "  q1 = cirq.NamedQubit('q1')\n",
        "  circuit = cirq.Circuit()\n",
        "  def basic_circuit(u0, u1):\n",
        "    if (u1 is not None):\n",
        "      yield u0(q0), u1(q1)\n",
        "    else:\n",
        "      yield u0(q0, q1)\n",
        "\n",
        "  is_next = True\n",
        "  while is_next:\n",
        "    idx = random.choice(range(gates.num_gates()))\n",
        "    circuit.append(basic_circuit(*gates._gates[idx]))\n",
        "    is_next = np.random.choice([True, False], p=[0.9, 0.1])\n",
        "\n",
        "  simulator = Simulator()\n",
        "  result = simulator.simulate(circuit, qubit_order=[q0, q1])\n",
        "\n",
        "  amplitudes = result.final_state_vector\n",
        "\n",
        "  # Check correctness of the new state\n",
        "  error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "  val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "  assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  # If I am equal to the initial state\n",
        "  if QuantumState(amplitudes).fidelity_score(QuantumState(np.array([1, 0, 0, 0]))) > (1-tolerance):\n",
        "    get_quantum_state(tolerance)\n",
        "\n",
        "  return amplitudes"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good? ==> in this sense no, but this is because of local minima. I do not have guarantee to find a global optimum with gradient descent (verify)\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, num_offline_updates, inference_ratio = 200):\n",
        "\n",
        "    self._env = Environment(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._keys)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, num_offline_updates)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "    self._successes = 0\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    inference_terminal_states = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      gates, terminal_state = self.run_inference(episode)\n",
        "      inference_gates.append(gates)\n",
        "      inference_terminal_states.append(terminal_state)\n",
        "\n",
        "    return inference_gates, inference_terminal_states\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "    # Add a reset step below, to take first action\n",
        "    self._agent.reset()\n",
        "\n",
        "\n",
        "  def run_inference(self, episode):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = []\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      gates.append(action)\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "\n",
        "    if not terminal:\n",
        "      return gates, self._env._state\n",
        "\n",
        "    max_gates = 20\n",
        "    if len(gates) < max_gates:\n",
        "      self._successes += 1\n",
        "      print(f\"Episode {episode}: inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates, self._env._terminal_state\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.1\n",
        "TRAINING_EPISODES = 100\n",
        "STEP_SIZE = 0.0001\n",
        "NUM_OFFLINE_UPDATES = 0 # Better remove it, I guess, we'll see\n",
        "NUM_EXPERIMENTS = 10"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbXs0upGaB2d"
      },
      "source": [
        "TODO:\n",
        "- Find a balance between number of gates and fidelity, to define best policy\n",
        "- Check fidelity score implementation correctness\n",
        "- Spiegare comportamento oscillatorio: secondo me dato lo spazio di ricerca cosi grande, il rischio è che esca facilmente da un local minimum la funzione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "ebfc270e-75b2-45b0-8358-d1c70a185639"
      },
      "source": [
        "%%time\n",
        "for i in range(NUM_EXPERIMENTS):\n",
        "  start = [1, 0, 0, 0]\n",
        "  target = get_quantum_state(TOLERANCE, i)\n",
        "\n",
        "  print(f\"Experiment {i}: target state = {np.around(target, 3)}\")\n",
        "\n",
        "  experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE, NUM_OFFLINE_UPDATES)\n",
        "  gates_sequences, terminal_states = experiment.run_experiment()\n",
        "  len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "  sns.heatmap(experiment._agent._W)\n",
        "  \n",
        "  idx = np.argmin(list(map(lambda x: len(x), gates_sequences)))\n",
        "  circuit_gates = gates_sequences[idx]\n",
        "  terminal_state = terminal_states[idx]\n",
        "  target_state = QuantumState(target)\n",
        "\n",
        "  print(target_state.fidelity_score(terminal_state))\n",
        "\n",
        "  compare_circuits(circuit_gates, terminal_state, target_state)\n",
        "  print(\"\")\n",
        "\n",
        "  plt.show()\n",
        "  print(\"\")\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment 0: target state = [0.389-0.678j 0.34 -0.022j 0.327+0.109j 0.307-0.245j]\n",
            "0.9217492293679032\n",
            "Custom gates approximate state: [0.655+0.193j 0.115+0.3j   0.038+0.581j 0.269+0.14j ]\n",
            "Google approximate state: [0.655+0.193j 0.115+0.3j   0.038+0.581j 0.269+0.14j ]\n",
            "Target state: [0.389-0.678j 0.34 -0.022j 0.327+0.109j 0.307-0.245j]\n",
            "Fidelity score: 0.9217492293679032\n",
            "\n",
            "q0: ───I───────────I───────────I───────────I────────────I────────────I───────────I───────────I───────────I───────────Rx(0.25π)───Rz(0.667π)───Rz(0.5π)───I────────────I───────────Rz(0.5π)───I────────────I───────────I───────────I───────────I───────────I───────────Rz(0.667π)───Rz(0.333π)───Rz(0.25π)───I────────────Rz(0.5π)───I────────────I───────────I───────────I───────────I───────────I───────────Rz(0.667π)───Rz(0.333π)───I────────────I───────────Rx(0.25π)───Rx(0.25π)───I────────────I────────────I───────I────────────Rx(0.667π)───I───────────@───I───────────I───────I───────────I───────────Rx(0.667π)───Rx(0.667π)───Rz(0.5π)───I───────────I───────────I───────I───────────I───────────Rx(0.667π)───Rx(0.667π)───I───────────I───────────I───────────I───────I───────────I───────────I───────────Rx(0.667π)───Rx(0.667π)───I───────────I───────────I───────────I───────────I───────I───────────I───────────I───────────I───────Rx(0.667π)───Rx(0.667π)───I───────I───────────I───────────Rx(0.667π)───Rx(0.25π)───I───────────I───────────Rx(0.667π)───I───────────I───────────I───────I───────────I───────────Rx(0.667π)───Rx(0.667π)───Rz(0.5π)───\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                │\n",
            "q1: ───Rx(0.25π)───Rx(0.25π)───Rx(0.25π)───Rz(0.333π)───Rx(0.667π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───I───────────I────────────I──────────Rz(0.333π)───Rx(0.25π)───I──────────Rz(0.333π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────I───────────Rz(0.333π)───I──────────Rz(0.333π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────Rx(0.667π)───Rz(0.25π)───I───────────I───────────Rx(0.667π)───Ry(0.333π)───Ry(π)───Rx(0.667π)───I────────────Ry(0.25π)───X───Ry(0.25π)───Ry(π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────I──────────Ry(0.25π)───Ry(0.25π)───Ry(π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────Ry(0.25π)───Ry(0.25π)───Ry(0.25π)───Ry(π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────Ry(0.25π)───Ry(0.25π)───Ry(0.25π)───Ry(0.25π)───Ry(π)───Rz(0.25π)───Rz(0.25π)───Rz(0.25π)───Ry(π)───I────────────I────────────Ry(π)───Rz(0.25π)───Rz(0.25π)───I────────────I───────────Ry(0.25π)───Ry(0.25π)───I────────────Ry(0.25π)───Ry(0.25π)───Ry(π)───Rz(0.25π)───Rz(0.25π)───I────────────I────────────I──────────\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Experiment 1: target state = [-0.234-0.135j -0.234-0.135j -0.327+0.566j -0.327+0.566j]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoEfGy_AxDmb"
      },
      "source": [
        "# Analysis and TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-DR4p_QJ8o"
      },
      "source": [
        "Count the number of successes.\n",
        "Some summary statistics of this type using binning should be plotted\n",
        "In general this could be useful to compare different polcies\n",
        "Please build a systematic set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6yyJGq1J9Oh"
      },
      "source": [
        "Compare \n",
        "- Step size decrease (che e un concettoesistente in Deep lr ==> scheduler): è migliorativo?\n",
        "- behavioural policy: random o eps greedy?\n",
        "- Different rewards\n",
        "- Ragionare su perché il miglioramente non è costante, come invece vorrei,\n",
        "==> ho bisogno di generalizzare? no devo essere di successo solo su cio che ho visto in training\n",
        "Behaviour policy always exploring has been solution: try without.\n",
        "Nota: la ragione del comportamento oscillante da episodio a episodio è dovuto al fatto che gradient descent rischia di cadere in local optima ==> rallenta la convergenza a una policy ottimale. Quanto invece ho dei buoni seed esco di lì. \n",
        "Devo mediare o cosa? Posso anche mediare, ma dire che a noi serve un global optimum per trovare l'optimal path, non generalizzare. Chiedo a tizio.\n",
        "\n",
        "- Comparo diverse target policy da diversi behaviour!\n",
        "- testare experience replay per capire al meglio ogni stato. ==> mi serve o posso togliere?\n",
        "- Comparo numero di policy di successo rispetto a un approccio random: però la policy che imparo è imparata, il random no, cretino ==> quindi anche se e peggio in generale, basta sia buono una volta. Probabilmente il minimo che trovo è solo locale, ma è gia sufficiente, e il fatto che ne esca continuamente richiede soluzione a piu ampio spettro a cui non sono interessato ora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7dwCD0J9Oi"
      },
      "source": [
        "Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ]
    }
  ]
}