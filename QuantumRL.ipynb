{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQOQswx05lsaB/v01cceqi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano frtemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "# TODO: compare results with qiskit\n",
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._mapping = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A11rxMP0zlf"
      },
      "source": [
        "# TODO: Compare simulation with qiskit results\n",
        "gates = Gates()\n",
        "q = QuantumState(np.array([0.4j, 0.3, 0.6, 0.624]))\n",
        "old_amplitudes = q.get_amplitudes()\n",
        "for g, name in zip(gates._gates, gates._mapping):\n",
        "  print(\"Gate \" + name)\n",
        "  q.apply_gate(g, inplace = True)\n",
        "  new_amplitudes = []\n",
        "  for val in q.get_amplitudes():\n",
        "    new_amplitudes.append(\"{:.3}\".format(val))\n",
        "\n",
        "  print(\"Applied gate \" + name + f\" to qubit with amplitudes {old_amplitudes}.\\n\" +\n",
        "        f\"Updated amplitudes: {new_amplitudes}\")\n",
        "  \n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._mapping, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._mapping (Dict[String: Int]): dicitonary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._mapping, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._current_fidelity = fidelity\n",
        "      self._state = next_state\n",
        "\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state\n",
        "    self._current_fidelity = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWp0UZ7q6VVF"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "  \n",
        "  def append_transition(self, transition):\n",
        "    self.buffer.append(transition)\n",
        "\n",
        "  def sample_transition(self):\n",
        "    return random.choice(self.buffer)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, num_offline_updates):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._state = initial_state\n",
        "    self._action = random.choice(range(number_of_actions))\n",
        "\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._replay_buffer = ReplayBuffer() # supervised dataset\n",
        "\n",
        "  def behaviour_policy(self, state):\n",
        "    # greedy = np.random.choice([True, False], p=[1-self._eps, self._eps])\n",
        "    # if greedy:\n",
        "    #   return np.argmax(self.q(state))\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    # Experience replay\n",
        "    self._replay_buffer.append_transition((s, a, r, g, next_s))\n",
        "    for _ in range(self._num_offline_updates):\n",
        "      s, a, r, g, next_s = self._replay_buffer.sample_transition()\n",
        "      self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy(next_s)\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade and Helper Functions\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good?\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor q value on 2D graphs.\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, num_offline_updates, inference_ratio = 200):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._mapping)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, num_offline_updates)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "    self._successes = 0\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      # if episode % self._inference_ratio == 0:\n",
        "      inference_gates.append(self.run_inference())\n",
        "\n",
        "    inference_gates.append(self.run_inference())\n",
        "\n",
        "    return inference_gates\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "\n",
        "  def run_inference(self):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "      gates.append(action)\n",
        "\n",
        "    max_gates = 20\n",
        "    if len(gates) >= max_gates:\n",
        "      # print(f\"Couldn't reach target in less than {max_gates} steps. Current fidelity {self._env._current_fidelity}.\")\n",
        "      return []\n",
        "\n",
        "    # self._agent._step_size *= .1\n",
        "    self._successes += 1\n",
        "    print(f\"Inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW1bekJrCTYu"
      },
      "source": [
        "# Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "# We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mlv4JNv2Vs7"
      },
      "source": [
        "Compare \n",
        "- Step size decrease (che e un concettoesistente in Deep lr ==> scheduler): è migliorativo?\n",
        "- behavioural policy: random o eps greedy?\n",
        "- Different rewards\n",
        "- Ragionare su perché il mglioramente non è costante, come invece vorrei,\n",
        "==> ho bisogno di generalizzare? no devo essere di successo solo su cio che ho visto in training\n",
        "Per fare cio pero mi serve esplorare il piu possibile? Non è detto, esplorazione nuova mi inficia\n",
        "su vecchi buoni risultati, magari peggiorando. penso questa sia la causa del comportameno oscillante.\n",
        "A questo proposito: per qualche ragione, mi da un comportamento anomalo (divisione in colonne dello stesso colore in agent._W) per eps < 1, ovvero per eps greedy policy. Perché?\n",
        "\n",
        "- Comparo diverse target policy da diversi behaviour!\n",
        "- testare experience replay per capire al meglio ogni stato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.2\n",
        "TRAINING_EPISODES = 5000\n",
        "STEP_SIZE = 0.0001\n",
        "NUM_OFFLINE_UPDATES = 10"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "15ee71a4-390f-448f-d404-485fcd158b44"
      },
      "source": [
        "target = [0.5, 0.5, 0.5, 0.5]\n",
        "start = [0.1j, 0.1, 0.1, -0.985]\n",
        "experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE, NUM_OFFLINE_UPDATES)\n",
        "gates_sequences = experiment.run_experiment()\n",
        "len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "print()\n",
        "print(experiment._successes)\n",
        "sns.heatmap(experiment._agent._W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference completed in 10 steps. Fidelity score: 0.8189792367620785\n",
            "Inference completed in 10 steps. Fidelity score: 0.8446380591744562\n",
            "Inference completed in 12 steps. Fidelity score: 0.8537701789798717\n",
            "Inference completed in 6 steps. Fidelity score: 0.8446380591744562\n",
            "Inference completed in 8 steps. Fidelity score: 0.8992533224293011\n",
            "Inference completed in 10 steps. Fidelity score: 0.9059021216744558\n",
            "Inference completed in 19 steps. Fidelity score: 0.8446380591744569\n",
            "Inference completed in 9 steps. Fidelity score: 0.8296033044824264\n",
            "Inference completed in 9 steps. Fidelity score: 0.8296033044824264\n",
            "Inference completed in 6 steps. Fidelity score: 0.8333701610329972\n",
            "Inference completed in 9 steps. Fidelity score: 0.8296033044824264\n",
            "Inference completed in 5 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 7 steps. Fidelity score: 0.8566521216744563\n",
            "Inference completed in 19 steps. Fidelity score: 0.856652121674456\n",
            "Inference completed in 19 steps. Fidelity score: 0.856652121674456\n",
            "Inference completed in 8 steps. Fidelity score: 0.904486371233187\n",
            "Inference completed in 5 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 9 steps. Fidelity score: 0.8296033044824264\n",
            "Inference completed in 9 steps. Fidelity score: 0.8296033044824264\n",
            "Inference completed in 8 steps. Fidelity score: 0.8652378272975324\n",
            "Inference completed in 16 steps. Fidelity score: 0.865366511800736\n",
            "Inference completed in 17 steps. Fidelity score: 0.829603304482426\n",
            "Inference completed in 6 steps. Fidelity score: 0.8333701610329972\n",
            "Inference completed in 9 steps. Fidelity score: 0.8154722522727671\n",
            "Inference completed in 19 steps. Fidelity score: 0.8057114373502344\n",
            "Inference completed in 8 steps. Fidelity score: 0.8652378272975324\n",
            "Inference completed in 9 steps. Fidelity score: 0.8652378272975328\n",
            "Inference completed in 15 steps. Fidelity score: 0.8968268243419597\n",
            "Inference completed in 10 steps. Fidelity score: 0.9634824539709852\n",
            "Inference completed in 11 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 6 steps. Fidelity score: 0.8333701610329972\n",
            "Inference completed in 10 steps. Fidelity score: 0.8154722522727671\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 11 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 8 steps. Fidelity score: 0.8834899164660801\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 10 steps. Fidelity score: 0.8154722522727664\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 10 steps. Fidelity score: 0.9059021216744562\n",
            "Inference completed in 16 steps. Fidelity score: 0.867588947686055\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 11 steps. Fidelity score: 0.8992533224293013\n",
            "Inference completed in 9 steps. Fidelity score: 0.8992533224293013\n",
            "Inference completed in 11 steps. Fidelity score: 0.9390398103108398\n",
            "Inference completed in 11 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 8 steps. Fidelity score: 0.8834899164660801\n",
            "Inference completed in 10 steps. Fidelity score: 0.9059021216744562\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 11 steps. Fidelity score: 0.9390398103108398\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 11 steps. Fidelity score: 0.9390398103108398\n",
            "Inference completed in 11 steps. Fidelity score: 0.856652121674456\n",
            "Inference completed in 10 steps. Fidelity score: 0.856652121674456\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 12 steps. Fidelity score: 0.9263923738203009\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 11 steps. Fidelity score: 0.9263923738203009\n",
            "Inference completed in 9 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 8 steps. Fidelity score: 0.8834899164660801\n",
            "Inference completed in 12 steps. Fidelity score: 0.9263923738203009\n",
            "Inference completed in 10 steps. Fidelity score: 0.8834899164660799\n",
            "Inference completed in 10 steps. Fidelity score: 0.8992533224293007\n",
            "Inference completed in 9 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 13 steps. Fidelity score: 0.9263923738203015\n",
            "Inference completed in 11 steps. Fidelity score: 0.9263923738203009\n",
            "Inference completed in 12 steps. Fidelity score: 0.8773010843954682\n",
            "Inference completed in 13 steps. Fidelity score: 0.8693232770595594\n",
            "Inference completed in 13 steps. Fidelity score: 0.8773010843954684\n",
            "Inference completed in 7 steps. Fidelity score: 0.8747106290543184\n",
            "Inference completed in 9 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744563\n",
            "Inference completed in 10 steps. Fidelity score: 0.9305271216744561\n",
            "Inference completed in 9 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 10 steps. Fidelity score: 0.8553009969585522\n",
            "Inference completed in 10 steps. Fidelity score: 0.8553009969585522\n",
            "Inference completed in 11 steps. Fidelity score: 0.8992533224293013\n",
            "Inference completed in 19 steps. Fidelity score: 0.8043858875552794\n",
            "Inference completed in 19 steps. Fidelity score: 0.8043858875552794\n",
            "Inference completed in 16 steps. Fidelity score: 0.8567459017320421\n",
            "Inference completed in 12 steps. Fidelity score: 0.8992533224293007\n",
            "Inference completed in 16 steps. Fidelity score: 0.8567459017320421\n",
            "Inference completed in 9 steps. Fidelity score: 0.8006184059016864\n",
            "Inference completed in 7 steps. Fidelity score: 0.8578293825601474\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n",
            "Inference completed in 18 steps. Fidelity score: 0.8446380591744562\n",
            "Inference completed in 16 steps. Fidelity score: 0.8051368449526981\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n",
            "Inference completed in 16 steps. Fidelity score: 0.8051368449526981\n",
            "Inference completed in 11 steps. Fidelity score: 0.8992533224293007\n",
            "Inference completed in 10 steps. Fidelity score: 0.8027339062996515\n",
            "Inference completed in 19 steps. Fidelity score: 0.829603304482426\n",
            "Inference completed in 19 steps. Fidelity score: 0.8296033044824258\n",
            "Inference completed in 10 steps. Fidelity score: 0.8215399074926428\n",
            "Inference completed in 19 steps. Fidelity score: 0.8296033044824258\n",
            "Inference completed in 19 steps. Fidelity score: 0.829603304482426\n",
            "Inference completed in 19 steps. Fidelity score: 0.8296033044824258\n",
            "Inference completed in 10 steps. Fidelity score: 0.8992533224293016\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n",
            "Inference completed in 10 steps. Fidelity score: 0.9551521216744561\n",
            "Inference completed in 17 steps. Fidelity score: 0.8101861323802895\n",
            "Inference completed in 17 steps. Fidelity score: 0.8154722522727661\n",
            "Inference completed in 17 steps. Fidelity score: 0.8154722522727661\n",
            "Inference completed in 12 steps. Fidelity score: 0.829603304482426\n",
            "Inference completed in 10 steps. Fidelity score: 0.8992533224293016\n",
            "Inference completed in 7 steps. Fidelity score: 0.8992533224293009\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n",
            "Inference completed in 8 steps. Fidelity score: 0.9702249999999998\n",
            "Inference completed in 10 steps. Fidelity score: 0.8296033044824266\n",
            "Inference completed in 9 steps. Fidelity score: 0.9445467839290738\n",
            "Inference completed in 9 steps. Fidelity score: 0.8215399074926426\n",
            "Inference completed in 18 steps. Fidelity score: 0.8644283134558634\n",
            "Inference completed in 14 steps. Fidelity score: 0.806000180281905\n",
            "Inference completed in 10 steps. Fidelity score: 0.8296033044824266\n",
            "Inference completed in 9 steps. Fidelity score: 0.9445467839290738\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n",
            "Inference completed in 19 steps. Fidelity score: 0.8023182652980198\n",
            "Inference completed in 19 steps. Fidelity score: 0.8500996145291536\n",
            "Inference completed in 17 steps. Fidelity score: 0.8526734812274047\n",
            "Inference completed in 18 steps. Fidelity score: 0.9575686705645691\n",
            "Inference completed in 17 steps. Fidelity score: 0.8395006566286406\n",
            "Inference completed in 9 steps. Fidelity score: 0.9445467839290738\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744562\n",
            "Inference completed in 17 steps. Fidelity score: 0.9634824539709854\n",
            "Inference completed in 17 steps. Fidelity score: 0.9634824539709854\n",
            "Inference completed in 9 steps. Fidelity score: 0.8215399074926426\n",
            "Inference completed in 12 steps. Fidelity score: 0.8346239742958307\n",
            "Inference completed in 17 steps. Fidelity score: 0.9702249999999998\n",
            "Inference completed in 12 steps. Fidelity score: 0.8346239742958307\n",
            "Inference completed in 9 steps. Fidelity score: 0.9538655632742101\n",
            "Inference completed in 18 steps. Fidelity score: 0.8296033044824261\n",
            "Inference completed in 13 steps. Fidelity score: 0.8086815452644822\n",
            "Inference completed in 8 steps. Fidelity score: 0.9059021216744564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5-DR4p_QJ8o"
      },
      "source": [
        "# Count the number of successes.\n",
        "# Some summary statistics of this type using binning should be plotted\n",
        "# In general this could be useful to compare different polcies\n",
        "# Please build a systematic set up\n",
        "lengths = 0\n",
        "for el in gates_sequences[-100:]:\n",
        "  if el != []:\n",
        "    lengths += 1\n",
        "lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ARrmXeh-kHu",
        "outputId": "bf3affc3-632d-4192-b5aa-4030443a3ef3"
      },
      "source": [
        ""
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nzu_kr3-fr7",
        "outputId": "f23d778b-5edf-4824-fae8-c1de0b0821d4"
      },
      "source": [
        "print(np.mean(lengths))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2c4fkW4QCC",
        "outputId": "bef57749-c973-4f16-f9fd-fef4ac2f59e5"
      },
      "source": [
        "experiment._env._terminal_state.get_amplitudes()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.70876626+0.01913417j, -0.38731851-0.04619398j,\n",
              "       -0.51588681-0.03314136j, -0.26780692+0.08001031j])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pss1hqFz3G_",
        "outputId": "cb708b77-5127-497e-d7f7-41595de71b87"
      },
      "source": [
        "experiment._env._terminal_state.fidelity_score(experiment._env._target_state)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.856652121674456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Tz6BuBybfY",
        "outputId": "6efea5e7-16c1-46f6-f54d-3730158abb5e"
      },
      "source": [
        "len(experiment.run_inference()[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}