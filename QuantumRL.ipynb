{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NV8sEgIgZisp",
        "hnWqxby5ZfY_",
        "kfmGXb2TZWpw"
      ],
      "authorship_tag": "ABX9TyOwdFuJCrd0LgKQQEKykA22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNsunVDOmIM2"
      },
      "source": [
        "# Google QuantumAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDU__H-6mNf9"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQgeiVZmK4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d47049-425d-495c-ea9a-f8b91cba2e1f"
      },
      "source": [
        "try:\n",
        "    import cirq\n",
        "except ImportError:\n",
        "    print(\"installing cirq...\")\n",
        "    !pip install --quiet cirq\n",
        "    print(\"installed cirq.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "installing cirq...\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 13.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 68.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 68.5MB/s \n",
            "\u001b[?25hinstalled cirq.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ05rJZRmPcL"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng8kY2AmmXQS"
      },
      "source": [
        "import cirq\n",
        "from cirq import Simulator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano frtemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._keys = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQvWLKKRgZR9"
      },
      "source": [
        "#### Google Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a4R9ebGj2rl"
      },
      "source": [
        "class GoogleGates: \n",
        "  def __init__(self, keys = list()):\n",
        "    PI = math.pi\n",
        "    self._keys = keys\n",
        "    self._gates = [\n",
        "                  (cirq.CNOT, None),\n",
        "                  (cirq.rx(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI)),\n",
        "                  (cirq.rx(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(2*PI/3)),\n",
        "                  (cirq.rx(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/2)),\n",
        "                  (cirq.rx(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/3)),\n",
        "                  (cirq.rx(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rx(PI/4)),\n",
        "                  (cirq.ry(PI), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI)),\n",
        "                  (cirq.ry(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(2*PI/3)),\n",
        "                  (cirq.ry(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/2)),\n",
        "                  (cirq.ry(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/3)),\n",
        "                  (cirq.ry(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.ry(PI/4)),\n",
        "                  (cirq.rz(PI), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI)),\n",
        "                  (cirq.rz(2*PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(2*PI/3)),\n",
        "                  (cirq.rz(PI/2), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/2)),\n",
        "                  (cirq.rz(PI/3), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/3)),\n",
        "                  (cirq.rz(PI/4), cirq.I),\n",
        "                  (cirq.I, cirq.rz(PI/4))\n",
        "    ]\n",
        "\n",
        "\n",
        "  def num_gates(self):\n",
        "    return len(self._gates)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._keys, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._keys (Dict[String: Int]): dicitonary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._keys, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._state = next_state\n",
        "\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWp0UZ7q6VVF"
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.buffer = []\n",
        "  \n",
        "  def append_transition(self, transition):\n",
        "    self.buffer.append(transition)\n",
        "\n",
        "  def sample_transition(self):\n",
        "    return random.choice(self.buffer)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, num_offline_updates):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._initial_state = initial_state\n",
        "\n",
        "    self._num_offline_updates = num_offline_updates\n",
        "    self._replay_buffer = ReplayBuffer() # supervised dataset\n",
        "\n",
        "    # St self._state to initial state and pick an action randomly\n",
        "    self.reset(True)\n",
        "\n",
        "\n",
        "  def behaviour_policy(self, state = None):\n",
        "    # greedy = np.random.choice([True, False], p=[1-self._eps, self._eps])\n",
        "    # if greedy:\n",
        "    #   return np.argmax(self.q(state))\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    # Experience replay\n",
        "    self._replay_buffer.append_transition((s, a, r, g, next_s))\n",
        "    for _ in range(self._num_offline_updates):\n",
        "      s, a, r, g, next_s = self._replay_buffer.sample_transition()\n",
        "      self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy()\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "\n",
        "  def reset(self, random = False):\n",
        "    self._state = self._initial_state\n",
        "    if random:\n",
        "      self._action = self.behaviour_policy()\n",
        "    else:\n",
        "      self._action = np.argmax(self.q(self._state))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade and Helper Functions\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good? ==> in this sense no, but this is because of local minima. I do not have guaranteee to find a global otimum with gradient descent (verify)\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, num_offline_updates, inference_ratio = 200):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._keys)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, num_offline_updates)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "    self._successes = 0\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    inference_terminal_states = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      gates, terminal_state = self.run_inference(episode)\n",
        "      inference_gates.append(gates)\n",
        "      inference_terminal_states.append(terminal_state)\n",
        "\n",
        "    return inference_gates, inference_terminal_states\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "\n",
        "      # Problem here: this action is taken randomly wrt to start state.\n",
        "      # This action taken randomly propagates to next inference step.\n",
        "      # Instead, I would like the first inference step to be take greedily.\n",
        "      # Add a reset step below, to take action greedily\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "    self._agent.reset()\n",
        "\n",
        "\n",
        "  def run_inference(self, episode):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "      if not terminal:\n",
        "        gates.append(action)\n",
        "\n",
        "    max_gates = 20\n",
        "    if len(gates) < max_gates:\n",
        "      self._successes += 1\n",
        "      print(f\"Episode {episode}: inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates, self._env._terminal_state\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwKia38JvwL"
      },
      "source": [
        "def compare_circuits(circuit_gates, final_state, target_state):\n",
        "  \"\"\"\n",
        "  Compare results obtained by google gates vs my custom gates implementation\n",
        "  Args:\n",
        "    circuit_gates (List[int]): indices of the gates in the circuit defined by the agent\n",
        "    final_state (QuantumState): terminal state reached by the agent\n",
        "    target_state (QuantumState): target state of TD agent\n",
        "  \"\"\"\n",
        "  my_gates = Gates()\n",
        "  google_gates = GoogleGates(my_gates._keys)\n",
        "\n",
        "  # Google initialization\n",
        "  q0 = cirq.NamedQubit('q0')\n",
        "  q1 = cirq.NamedQubit('q1')\n",
        "  circuit = cirq.Circuit()\n",
        "  def basic_circuit(u0, u1):\n",
        "    if (u1 is not None):\n",
        "      yield u0(q0), u1(q1)\n",
        "    else:\n",
        "      yield u0(q0, q1)\n",
        "\n",
        "  q = QuantumState(np.array([1, 0, 0, 0]))\n",
        "  for idx in circuit_gates:\n",
        "    q = q.apply_gate(my_gates._gates[idx])\n",
        "    circuit.append(basic_circuit(*google_gates._gates[idx]))\n",
        "\n",
        "  simulator = Simulator()\n",
        "  result = simulator.simulate(circuit, qubit_order=[q0, q1])\n",
        "\n",
        "  print(f\"Approximate state: {np.around(q.get_amplitudes(), 3)}\")\n",
        "  print(f\"Target state: {np.around(target_state.get_amplitudes(), 3)}\")\n",
        "\n",
        "  assert np.all(final_state.get_amplitudes() == q.get_amplitudes()),\\\n",
        "   f\"Unexpected terminal state: got {np.around(q.get_amplitudes(), 3)} rather than {np.around(final_state.get_amplitudes(), 3)}\"\n",
        "\n",
        "  # assert np.all(np.around(result.final_state_vector, 3) == np.around(final_state.get_amplitudes(), 3)),\\\n",
        "  #  f\"Google and custom gates return differents results: got {np.around(result.final_state_vector, 3)} and {np.around(final_state.get_amplitudes(), 3)}\"\n",
        "\n",
        "  print(\"\")\n",
        "  print(circuit)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyWWqvKkQSMj"
      },
      "source": [
        "def get_quantum_state(tolerance):\n",
        "  \"\"\"\n",
        "  Get a feasible set of amplitudes of a quantum state, \n",
        "  by applying a random number of random gates to the initial state |00>\n",
        "\n",
        "  Return:\n",
        "    amplitudes (np.array): list of new state amplitudes\n",
        "  \"\"\"\n",
        "\n",
        "  gates = GoogleGates()\n",
        "\n",
        "  # Circuit initialization\n",
        "  q0 = cirq.NamedQubit('q0')\n",
        "  q1 = cirq.NamedQubit('q1')\n",
        "  circuit = cirq.Circuit()\n",
        "  def basic_circuit(u0, u1):\n",
        "    if (u1 is not None):\n",
        "      yield u0(q0), u1(q1)\n",
        "    else:\n",
        "      yield u0(q0, q1)\n",
        "\n",
        "  is_next = np.random.choice([True, False], p=[0.9, 0.1])\n",
        "  while is_next:\n",
        "    idx = random.choice(range(gates.num_gates()))\n",
        "    circuit.append(basic_circuit(*gates._gates[idx]))\n",
        "    is_next = np.random.choice([True, False], p=[0.9, 0.1])\n",
        "\n",
        "  simulator = Simulator()\n",
        "  result = simulator.simulate(circuit, qubit_order=[q0, q1])\n",
        "\n",
        "  amplitudes = result.final_state_vector\n",
        "\n",
        "  # Check correctness of the new state\n",
        "  error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "  val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "  assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  # If I am equal to the initial state\n",
        "  if QuantumState(amplitudes).fidelity_score(QuantumState(np.array([1, 0, 0, 0]))) > (1-tolerance):\n",
        "    get_quantum_state()\n",
        "\n",
        "  return amplitudes"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.1\n",
        "TRAINING_EPISODES = 50\n",
        "STEP_SIZE = 0.0001\n",
        "NUM_OFFLINE_UPDATES = 0 # Better remove it, I guess, we'll see\n",
        "NUM_EXPERIMENTS = 1"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbXs0upGaB2d"
      },
      "source": [
        "TODO:\n",
        "- Fix the sns plot\n",
        "- Find a balance between number of gates and fidelity, to define best policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "ec279b60-b623-4a15-cdea-858edd2ff452"
      },
      "source": [
        "%%time\n",
        "\n",
        "for i in range(NUM_EXPERIMENTS):\n",
        "  start = [1, 0, 0, 0]\n",
        "  target = get_quantum_state(TOLERANCE)\n",
        "\n",
        "  print(f\"Experiment {i}: target state = {np.around(target, 3)}\")\n",
        "\n",
        "  experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE, NUM_OFFLINE_UPDATES)\n",
        "  gates_sequences, terminal_states = experiment.run_experiment()\n",
        "  len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "  sns.heatmap(experiment._agent._W)\n",
        "  \n",
        "  idx = np.argmin(list(map(lambda x: len(x), gates_sequences)))\n",
        "  circuit_gates = gates_sequences[idx]\n",
        "  terminal_state = terminal_states[idx]\n",
        "  target_state = QuantumState(target)\n",
        "\n",
        "  print(target_state.fidelity_score(terminal_state))\n",
        "\n",
        "  compare_circuits(circuit_gates, terminal_state, target_state)\n",
        "  print(\"\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment 0: target state = [-0.486-0.048j  0.354-0.106j -0.589+0.225j  0.348-0.327j]\n",
            "Episode 8: inference completed in 11 steps. Fidelity score: 0.9392610633950229\n",
            "Episode 15: inference completed in 12 steps. Fidelity score: 0.951825809608805\n",
            "0.9392610633950229\n",
            "Approximate state: [-0.457+0.166j  0.264-0.096j -0.595+0.4j    0.344-0.231j]\n",
            "Final state: [-0.457+0.166j  0.264-0.096j -0.595+0.4j    0.344-0.231j]\n",
            "Target state: [-0.486-0.048j  0.354-0.106j -0.589+0.225j  0.348-0.327j]\n",
            "\n",
            "q0: ───I────────────Ry(0.25π)───Ry(0.25π)───Ry(0.25π)───I────────────Rx(π)───Ry(0.667π)───Rz(0.667π)───Rx(π)───Ry(0.667π)───I────────────\n",
            "\n",
            "q1: ───Ry(0.333π)───I───────────I───────────I───────────Rz(0.333π)───I───────I────────────I────────────I───────I────────────Rz(0.667π)───\n",
            "\n",
            "CPU times: user 14.1 s, sys: 1.42 s, total: 15.6 s\n",
            "Wall time: 13.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoEfGy_AxDmb"
      },
      "source": [
        "# Analysis and TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5-DR4p_QJ8o"
      },
      "source": [
        "Count the number of successes.\n",
        "Some summary statistics of this type using binning should be plotted\n",
        "In general this could be useful to compare different polcies\n",
        "Please build a systematic set up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6yyJGq1J9Oh"
      },
      "source": [
        "Compare \n",
        "- Step size decrease (che e un concettoesistente in Deep lr ==> scheduler): è migliorativo?\n",
        "- behavioural policy: random o eps greedy?\n",
        "- Different rewards\n",
        "- Ragionare su perché il miglioramente non è costante, come invece vorrei,\n",
        "==> ho bisogno di generalizzare? no devo essere di successo solo su cio che ho visto in training\n",
        "Behaviour policy always exploring has been solution: try without.\n",
        "Nota: la ragione del comportamento oscillante da episodio a episodio è dovuto al fatto che gradient descent rischia di cadere in local optima ==> rallenta la convergenza a una policy ottimale. Quanto invece ho dei buoni seed esco di lì. \n",
        "Devo mediare o cosa? Posso anche mediare, ma dire che a noi serve un global optimum per trovare l'optimal path, non generalizzare. Chiedo a tizio.\n",
        "\n",
        "- Comparo diverse target policy da diversi behaviour!\n",
        "- testare experience replay per capire al meglio ogni stato. ==> mi serve o posso togliere?\n",
        "- Comparo numero di policy di successo rispetto a un approccio random: però la policy che imparo è imparata, il random no, cretino ==> quindi anche se e peggio in generale, basta sia buono una volta. Probabilmente il minimo che trovo è oslo locale, ma è gia sufficiente, e il fatto che ne esca continuamente richiede soluzione a piu ampio spettro a cui non sono interessato ora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj7dwCD0J9Oi"
      },
      "source": [
        "Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ]
    }
  ]
}