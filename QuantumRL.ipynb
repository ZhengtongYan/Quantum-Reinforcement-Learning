{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuantumRL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/0vH9OvMYlbcI489zq+Yy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescomontagna/Quantum-Reinforcement-Learning/blob/main/QuantumRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ywYaaLPmtL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGJGzcDIxwC"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import cmath\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from functools import reduce"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWCQSEmAOUI3"
      },
      "source": [
        "# Introduction\n",
        "## Problems\n",
        "- Since we can access the features, and know the transition probability from a state to another, this RL protocol is model based.\n",
        "Note that this is in contrast with the uncertainty about a quantum state from the observator point of view: an observator can only access the collapsed state, having no access to the amplitudes. \n",
        "\n",
        "In the paper Girolami sent me, they explicitly account for this fact, setting up a model free protocol.  \n",
        "\n",
        "\n",
        "## Reward\n",
        "Il principale problema è la formulzione della reward. per ora, l'unica soluzione tale da portare risultati accettabili è stata:\n",
        "- reward = \"grande\" per stato terminal\n",
        "- reward < 0 per stato non terminal  \n",
        "\n",
        "L'aggiunta di reward negativa a punire ogni step che non portasse a uno stato terminale, è stato cruciale. Infatti, ho usato altri tipi di reward, ma tutte fallimentari  \n",
        "- fidelity\n",
        "- fidelity per stato terminale, altrimenti 0\n",
        "- \"grande\" per stato terminale, altrimenti 0  \n",
        "\n",
        "Tutti questi tentativi si sono mostrati fallimentari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_8rsT6Mr7Zy"
      },
      "source": [
        "## Off Policy\n",
        "Sono dovuto ricorrere a un'approccio off policy, in quanto un approccio on policy non garantiva esplorazione sufficiente in uno spazio di ricerca così vasto. I risultati erano frtemente limitati, la matrice dei weight non vedeva mai alcune azioni, il cui valore rimaneva 0 per ogni features. I risultati erano in media peggiori di un' approccio Random.  \n",
        "Andando a usare una bahaviour policy completamente randomica, e una target policy completamente greedy, i risultati sono notavolmente migliorati. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LCGYpUPpTm"
      },
      "source": [
        "# Quantum Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8sEgIgZisp"
      },
      "source": [
        "### Qubit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjboXlFWG5EK"
      },
      "source": [
        "class Qubit:\n",
        "  def __init__(self, amplitudes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      amplitudes (np.array): amplitudes of the |0>, |1> vectors\n",
        "    \"\"\"\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    assert math.isclose(reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes)), 1, rel_tol = 1e-2), error\n",
        "    self._amplitudes = amplitudes\n",
        "\n",
        "  def qubit(self):\n",
        "    return self._amplitudes\n",
        "    \n",
        "\n",
        "class Basis(Qubit):\n",
        "  def __init__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      index: index of the position = 1\n",
        "    \"\"\"\n",
        "    amplitudes = np.zeros((2, ))\n",
        "    amplitudes[index] += 1\n",
        "    super().__init__(amplitudes)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnWqxby5ZfY_"
      },
      "source": [
        "### Quantum State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBaWNuZyhgp"
      },
      "source": [
        "class QuantumState:\n",
        "  def __init__(self, amplitudes): \n",
        "\n",
        "    # We assume computational basis\n",
        "    self._basis = {\n",
        "        '00': [Basis(0), Basis(0)],\n",
        "        '01': [Basis(0), Basis(1)],\n",
        "        '10': [Basis(1), Basis(0)],\n",
        "        '11': [Basis(1), Basis(1)],\n",
        "    }\n",
        "\n",
        "    a1, a2, a3, a4 = amplitudes\n",
        "    self._amplitudes = {\n",
        "        '00': a1,\n",
        "        '01': a2,\n",
        "        '10': a3,\n",
        "        '11': a4\n",
        "    }\n",
        "    \n",
        "    self._keys = list(self._basis.keys())\n",
        "\n",
        "    error = \"Error: sum of squared amplitudes must be = 1\"\n",
        "    val = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, amplitudes))\n",
        "    assert math.isclose(val, 1, rel_tol = 1e-2), error + f\" instead is {val}\"\n",
        "\n",
        "  def get_amplitudes(self):\n",
        "    return np.array(list(self._amplitudes.values()))\n",
        "\n",
        "  def get_features(self):\n",
        "    real = np.real(list(self._amplitudes.values()))\n",
        "    imag = np.imag(list(self._amplitudes.values()))\n",
        "    return np.concatenate((real, imag))\n",
        "\n",
        "  def apply_gate(self, gate, inplace = False):\n",
        "    updated_amplitudes = gate.apply(self)\n",
        "    if inplace:\n",
        "      self._amplitudes = updated_amplitudes\n",
        "      return None\n",
        "\n",
        "    return QuantumState(list(updated_amplitudes.values()))\n",
        "\n",
        "  def fidelity_score(self, other):\n",
        "    # TODO: check on nielsen, implement well.\n",
        "    # This implementation is from paper\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      other (QuantumState): measure the fidelity between self and another quantum state\n",
        "    Return:\n",
        "      fidelity (float): fidelity score between [0, 1]\n",
        "    \"\"\"\n",
        "    # Inner product can be computed in terms of matrix representation. Page 67 Nielsen-Chuang\n",
        "    return np.square(abs(np.matmul(np.conj(self.get_amplitudes()), other.get_amplitudes())))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboUEAK3ZbWt"
      },
      "source": [
        "### Quantum Gates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOc4Ouiockr"
      },
      "source": [
        "class QuantumGate:\n",
        "  def __init__(self, name, unitary, target):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      unitary: 2x2 unitary operator\n",
        "      target: 0 or 1 to denote the qubit the matrix is acting on\n",
        "    \"\"\"\n",
        "    self._name = name\n",
        "    self._U = unitary.flatten()\n",
        "    self._target = target\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      quantum_state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      updated (dict): didctionary with updated amplitudes\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._target == 0:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = self._U[2]*a_00 + self._U[3]*a_10\n",
        "      updated['11'] = self._U[2]*a_01 + self._U[3]*a_11\n",
        "\n",
        "    else:\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_01\n",
        "      updated['01'] = self._U[2]*a_00 + self._U[3]*a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated\n",
        "\n",
        "\n",
        "class CNOT(QuantumGate):\n",
        "  def __init__(self, control):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      control (int): if 0, first qubit is the control, else second\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    self._control = control\n",
        "    super().__init__('cnot', np.array([[0, 1], [1, 0]]), 1-control)\n",
        "\n",
        "\n",
        "  def apply(self, quantum_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (QuantumState): quantum state to which apply the quantum operator\n",
        "    Return:\n",
        "      result (QuantumState): quantum state with amplitudes modified\n",
        "    \"\"\"\n",
        "    # Directly implement update rule on the amplitudes for a 2 qubits case. \n",
        "    # NOTE: This approach is not scalable! Refine or use qiskit for more than 2 qubits\n",
        "    a_00, a_01, a_10, a_11 = quantum_state.get_amplitudes()\n",
        "    updated = dict()\n",
        "\n",
        "    if self._control == 0: # Then target = 2nd: if 1st qubit == 1, flip 2nd qubit.\n",
        "      updated['00'] = a_00\n",
        "      updated['01'] = a_01\n",
        "      updated['10'] = self._U[0]*a_10 + self._U[1]*a_11\n",
        "      updated['11'] = self._U[2]*a_10 + self._U[3]*a_11\n",
        "\n",
        "    if self._control == 1: # Then control = 1st: if 2nd qubit == 1, flip 1st qubit.\n",
        "      updated['00'] = self._U[0]*a_00 + self._U[1]*a_10\n",
        "      updated['01'] = self._U[0]*a_01 + self._U[1]*a_11\n",
        "      updated['10'] = a_10\n",
        "      updated['11'] = a_11\n",
        "\n",
        "    # Check if amplitudes still satisfy condition\n",
        "    normalization = reduce(lambda a, b: a+b, map(lambda a: abs(a)**2, updated.values()))\n",
        "    error = f\"Error: sum of squared amplitudes must be = 1.\\n Amplitudes: {list(updated.values())}, summing up to {normalization}\"\n",
        "    assert math.isclose(normalization, 1, rel_tol = 1e-2), error\n",
        "    \n",
        "    return updated"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmGXb2TZWpw"
      },
      "source": [
        "### Gates List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pus5TovHiETW"
      },
      "source": [
        "# TODO: compare results with qiskit\n",
        "class Gates:\n",
        "  def __init__(self):\n",
        "    gates = dict()\n",
        "    self._num_gates = 0\n",
        "\n",
        "    # Useful\n",
        "    targets = [0, 1]\n",
        "    j = 1j # complex unit\n",
        "\n",
        "    ########### CNOT ###########\n",
        "    gates['CNOT'] = CNOT(control = 0)\n",
        "    self._num_gates += 1\n",
        "\n",
        "    ########### Rotations ###########\n",
        "    angles_names = ['pi', '2pi/3', 'pi/2', 'pi/3', 'pi/4']\n",
        "    angles_values = (math.pi / 2) * np.array([1, 2/3, 1/2, 1/3, 1/4])\n",
        "    angles = {k:v for k,v in zip(angles_names, angles_values)}\n",
        "\n",
        "    # Rx\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rx' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -j*math.sin(theta)],\n",
        "                                                      [-j*math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "        \n",
        "    # Ry\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Ry' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[math.cos(theta), -math.sin(theta)],\n",
        "                                                      [math.sin(theta), math.cos(theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    # Rz\n",
        "    for name, theta in angles.items():\n",
        "      for t in targets:\n",
        "        key = 'Rz' + str(t) + '(' + name + ')'\n",
        "        gates[key] = QuantumGate(key, np.array([[cmath.exp(-j*theta), 0],\n",
        "                                                      [0, cmath.exp(j*theta)]]), t)\n",
        "        self._num_gates += 1\n",
        "\n",
        "    self._mapping = list(gates.keys())\n",
        "    self._gates = list(gates.values())\n",
        "\n",
        "  def num_gates(self):\n",
        "    return self._num_gates"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A11rxMP0zlf"
      },
      "source": [
        "# TODO: Compare simulation with qiskit results\n",
        "gates = Gates()\n",
        "q = QuantumState(np.array([0.4j, 0.3, 0.6, 0.624]))\n",
        "old_amplitudes = q.get_amplitudes()\n",
        "for g, name in zip(gates._gates, gates._mapping):\n",
        "  print(\"Gate \" + name)\n",
        "  q.apply_gate(g, inplace = True)\n",
        "  new_amplitudes = []\n",
        "  for val in q.get_amplitudes():\n",
        "    new_amplitudes.append(\"{:.3}\".format(val))\n",
        "\n",
        "  print(\"Applied gate \" + name + f\" to qubit with amplitudes {old_amplitudes}.\\n\" +\n",
        "        f\"Updated amplitudes: {new_amplitudes}\")\n",
        "  \n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJvflImPuhP"
      },
      "source": [
        "# RL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflbZWf75E7C"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL6bfPeCJSbl"
      },
      "source": [
        "# LinearModel of the environment\n",
        "class LinearModel:\n",
        "  def __init__(self, initial_state, target_state, tolerance):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state (QuantumState): initial state of the system\n",
        "      target_state (QuantumState): target state of the system\n",
        "      tolerance (float): tolerance in terms of fidelity score\n",
        "    \"\"\"\n",
        "    self._initial_state = initial_state\n",
        "    self._target_state = target_state\n",
        "    self._tolerance = tolerance # tolerance in terms of fidelity between\n",
        "    self._quantum_gates, self._mapping, self._num_gates = self.gates_set()\n",
        "    self._terminal_fidelity = 0 # used to retrieve the info at the end of an episode\n",
        "    self._terminal_state = None # used to retrieve the info at the end of an episode\n",
        "    \n",
        "    # Used to initialize env from scratch.\n",
        "    self.reset() \n",
        "\n",
        "    assert initial_state.fidelity_score(target_state) < (1-tolerance), f\"The two state are the same up to {tolerance} tolerance\"\n",
        "\n",
        "    \n",
        "  def gates_set(self):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      gates._gates (List[QuantumGate]): return the list with available QuantumGate objects\n",
        "      gates._mapping (Dict[String: Int]): dicitonary mapping a gate to its action index\n",
        "      gates.num_gates() (int): number of available gates (actions)\n",
        "    \"\"\"\n",
        "    gates = Gates()\n",
        "    return gates._gates, gates._mapping, gates.num_gates()\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Given a gate, apply it to self._state.\n",
        "    Set the reward, resulting state and discount.\n",
        "    Return these values, along with the information if therminal state has been reached\n",
        "    Args:\n",
        "      action (int): action index to select a QuantumGate\n",
        "\n",
        "    Return:\n",
        "      reward, discount, next_state features, terminal\n",
        "    \"\"\"\n",
        "    # Get and apply action. next_state is a QuantumState\n",
        "    gate = self._quantum_gates[action]\n",
        "    next_state = self._state.apply_gate(gate)\n",
        "\n",
        "    # Compare new state and target\n",
        "    fidelity = next_state.fidelity_score(self._target_state)\n",
        "\n",
        "    # Assign reward based on state and fidelity\n",
        "    terminal = self.is_terminal(fidelity)\n",
        "\n",
        "    # Terminal state\n",
        "    if terminal:\n",
        "      reward = +100.\n",
        "      discount = 0.\n",
        "      self._terminal_fidelity = fidelity\n",
        "      self._terminal_state = next_state\n",
        "      self.reset()\n",
        "    else:\n",
        "      reward = -6.\n",
        "      discount = 0.9\n",
        "      self._current_fidelity = fidelity\n",
        "      self._state = next_state\n",
        "\n",
        "\n",
        "    # Return the features, not the state itself\n",
        "    return reward, discount, self.get_obs(), terminal\n",
        "\n",
        "\n",
        "  def is_terminal(self, fidelity):\n",
        "    \"\"\"\n",
        "    Check if, by a level of self.tolerance, state is terminal\n",
        "    \"\"\"\n",
        "    if fidelity > (1 - self._tolerance):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "  def get_obs(self):\n",
        "    return self._state.get_features()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self._state = self._initial_state\n",
        "    self._current_fidelity = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZoAqcy3qkH"
      },
      "source": [
        "## Agent\n",
        "There is a bug I think, when I provide as next_state the initial state I\n",
        "- agent is in state self._state: a state contiguous to the terminal one, T, that we call S\n",
        "- the update is done for S wrt to next_state, which in this case is I, and not T as expected. There are 2 major drawbacks as consequence  \n",
        "One is that the update is done wrongly: r + g*q(I), but I here has not any sense\n",
        "Two is that I will never learn that this state is contiguous.\n",
        "\n",
        "Now, I have to reason about this, because I think in the assignment they used this approach, but better to write down this doubt.\n",
        "\n",
        "NO! This issue is fixed by putting discount = 0. Alright :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De-sHIvHwKra"
      },
      "source": [
        "# Least Square TD Agent: action value function approximation\n",
        "# implemented with gradient descent.\n",
        "class LSTDAgent:\n",
        "  def __init__(self, number_of_actions, number_of_features,\n",
        "      initial_state, step_size, eps):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      initial_state: it is a feature vector!\n",
        "    \"\"\"\n",
        "    self._number_of_actions = number_of_actions\n",
        "    self._W = np.zeros((number_of_actions, number_of_features))\n",
        "    self._step_size = step_size\n",
        "    self._state = initial_state\n",
        "    self._action = random.choice(range(number_of_actions))\n",
        "    self._eps = eps\n",
        "\n",
        "  def behaviour_policy(self, state):\n",
        "    greedy = np.random.choice([True, False], p=[1-self._eps, self._eps])\n",
        "    if greedy:\n",
        "      return np.argmax(self.q(state))\n",
        "    return random.choice(range(self._number_of_actions))\n",
        "\n",
        "  def q(self, state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      state (list): list of the amplitudes := features of the state\n",
        "    \"\"\"\n",
        "    # TODO: chiedere a davide per le features...\n",
        "    return np.matmul(self._W, state)\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      next_state (list): list of the amplitude: these are already features, not a QuantumState instance\n",
        "      terminal (boolean): if next_state is the terminal state\n",
        "    \"\"\"\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    # WARNING: If s is complex, also the update become complex ... Can I decide to have complex weights?\n",
        "    self._W[a] += self._step_size * (r + g * np.max(self.q(next_s)) - self.q(s)[a]) * s\n",
        "\n",
        "    next_a = self.behaviour_policy(next_s)\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a\n",
        "\n",
        "  def inference(self, reward, discount, next_state):\n",
        "    s = self._state\n",
        "    a = self._action\n",
        "    r = reward\n",
        "    next_s = next_state\n",
        "    g = discount\n",
        "\n",
        "    next_a = np.argmax(self.q(next_s))\n",
        "    self._action = next_a\n",
        "    self._state = next_s\n",
        "\n",
        "    return next_a"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFL1yPnM06xH"
      },
      "source": [
        "# Experiment Facade and Helper Functions\n",
        "Class to handle experiments and visualization  \n",
        "- [X] run experiment\n",
        "- [ ] Allow for agent.q visualization for a state, providing labels to actions. This allow to see if very close states are associated to very close actions\n",
        "- [ ] Monitor number of steps as experience grow: is the agent actually learning good?\n",
        "- [ ] Monitor the weights (see if I can find any meaning): need to put labels about actions ==> does an action focuses on an amplitudes subset as I would expect (e.g. if an amplitude is not touched by  gate, than I expect its weight value to be low\n",
        "- [X] Monitor q value on 2D graphs.\n",
        "- [X] Monitor fidelity score inside episodes --> add inference every N steps\n",
        "- [ ] Monitor impact of initial gate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfMBR8Ws0sqy"
      },
      "source": [
        "# Facade class to run experiment\n",
        "class Experiment:\n",
        "  def __init__(self, start_state, target_state, tolerance, number_of_episodes, step_size, eps, inference_ratio = 200):\n",
        "\n",
        "    self._env = LinearModel(QuantumState(start_state), QuantumState(target_state), tolerance)\n",
        "\n",
        "    number_of_actions = len(self._env._mapping)\n",
        "    initial_features = self._env.get_obs()\n",
        "\n",
        "    self._agent = LSTDAgent(number_of_actions, len(initial_features), initial_features,\n",
        "                            step_size, eps)\n",
        "    \n",
        "    self._number_of_episodes = number_of_episodes\n",
        "\n",
        "    self._episodes_gates = [] # check disrtribution of gates in different solutions\n",
        "    self._mean_rewards = [] # list of mean reward for each episode\n",
        "    self._inference_ratio = inference_ratio # run an inference episode every inference_ratio training episodes\n",
        "\n",
        "\n",
        "  def run_experiment(self):\n",
        "    \"\"\"\n",
        "    Run episodes, gathering statistics and updating user on conosole.\n",
        "    \"\"\"\n",
        "    # Run inference on 1 out of inference_ratio training episodes\n",
        "    inference_gates = []\n",
        "    for episode in range(self._number_of_episodes):\n",
        "      self.run_episode()\n",
        "      if episode % self._inference_ratio == 0:\n",
        "        inference_gates.append(self.run_inference())\n",
        "\n",
        "    inference_gates.append(self.run_inference())\n",
        "\n",
        "    return inference_gates\n",
        "\n",
        "\n",
        "  def run_episode(self):\n",
        "    \"\"\"\n",
        "    Run a single episode.\n",
        "    At the beginning of an episode we must guarantee\n",
        "    - initial state in the environment\n",
        "    - initial state in the agent\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    while not terminal:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.step(reward, discount, next_s)\n",
        "\n",
        "\n",
        "  def run_inference(self):\n",
        "    \"\"\"\n",
        "    Run an episode using the optimal policy learned\n",
        "    \"\"\"\n",
        "    terminal = False\n",
        "    action = self._agent._action\n",
        "    gates = [action]\n",
        "    while not terminal and len(gates) <= 1000:\n",
        "      reward, discount, next_s, terminal = self._env.step(action)\n",
        "      action = self._agent.inference(reward, discount, next_s)\n",
        "      gates.append(action)\n",
        "\n",
        "    if len(gates) >= 100:\n",
        "      print(f\"Couldn't reach target in less than 100 steps. Current fidelity {self._env._current_fidelity}.\")\n",
        "      return []\n",
        "\n",
        "    self._agent._step_size *= .1\n",
        "    print(f\"Inference completed in {len(gates)} steps. Fidelity score: {self._env._terminal_fidelity}\")\n",
        "\n",
        "    return gates\n",
        "\n",
        "  \n",
        "  def q_values(self):\n",
        "    \"\"\"\n",
        "    The function should plot, in a [0, 1] complex plane, the value function\n",
        "    on the z axis, for each of the 4 value of amplitude.\n",
        "    In order to get reasonable values and understand the effect of the amplitude\n",
        "    under analysis on the overall q_values, the remaining amplitudes values will \n",
        "    be set to the target amplitudes.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
        "\n",
        "    state = self._env._target_state\n",
        "    features = state.get_features()\n",
        "    x = np.linspace(0, 1, num=100)\n",
        "    y = np.linspace(0, 1, num=100)\n",
        "    grid = np.meshgrid\n",
        "    q = []\n",
        "    for i in x:\n",
        "      row = []\n",
        "      for k in y:\n",
        "        features[0] = i\n",
        "        features[0] = k\n",
        "        row.append(self._agent.q(features))\n",
        "      \n",
        "      q.append(row)\n",
        "\n",
        "    q = np.array(q)\n",
        "    surf = ax.plot_surface(x, y, np.squeeze(q[:, :, 0]), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "    \n",
        "    # Add a color bar which maps values to colors.\n",
        "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKa4MYQ0tUp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN0SmXxG_LPx"
      },
      "source": [
        "- Need to tune all the parameters below. In particular, it is important to understand upt to which point we can decrease the tolerance.\n",
        "- Substitute TRAINING_EPISODES with number of training steps "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R5nEorB3kgW"
      },
      "source": [
        "TOLERANCE = 0.2\n",
        "TRAINING_EPISODES = 1000\n",
        "STEP_SIZE = 0.0001\n",
        "EPS = 1"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW1bekJrCTYu"
      },
      "source": [
        "# Run an inference every N episodes and monitor the optimality of the behaviour\n",
        "# We could add experience replay, since after a certain number of new episodes it doesn't work well anymore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mlv4JNv2Vs7"
      },
      "source": [
        "Compare \n",
        "- Step size decrease (che e un concettoesistente in Deep lr ==> scheduler): è migliorativo?\n",
        "- behavioural policy: random o eps greedy?\n",
        "- Different rewards\n",
        "- Ragionare su perché il mglioramente non è costante, come invece vorrei,\n",
        "==> ho bisogno di generalizzare? no devo essere di successo solo su cio che ho visto in training\n",
        "Per fare cio pero mi serve esplorare il piu possibile? Non è detto, esplorazione nuova mi inficia\n",
        "su vecchi buoni risultati, magari peggiorando. penso questa sia la causa del comportameno oscillante.\n",
        "A questo proposito: per qualche ragione, mi da un comportamento anomalo (divisione in colonne dello stesso colore in agent._W) per eps < 1, ovvero per eps greedy policy. Perché?\n",
        "\n",
        "- Comparo diverse target policy da diversi behaviour!\n",
        "- testare experience replay per capire al meglio ogni stato."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "7b2pDqPiVFxp",
        "outputId": "4526ff93-531f-4d81-e071-1c3d14987bd7"
      },
      "source": [
        "start = [0.1j, 0.1, 0.1, -0.985]\n",
        "target = [0.5, 0.5, 0.5, 0.5]\n",
        "experiment = Experiment(start, target, TOLERANCE, TRAINING_EPISODES, STEP_SIZE, EPS)\n",
        "gates_sequences = experiment.run_experiment()\n",
        "len_sequences = list(map(lambda x: len(x), gates_sequences))\n",
        "print()\n",
        "print(experiment._agent._step_size)\n",
        "sns.heatmap(experiment._agent._W)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inference completed in 33 steps. Fidelity score: 0.9585910751083772\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.35593501037582814.\n",
            "Couldn't reach target in less than 100 steps. Current fidelity 0.24585205390461387.\n",
            "Inference completed in 14 steps. Fidelity score: 0.8912139347426484\n",
            "Inference completed in 27 steps. Fidelity score: 0.8930428062246754\n",
            "Inference completed in 17 steps. Fidelity score: 0.8767272787132828\n",
            "\n",
            "1.0000000000000004e-08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff37f861c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-DR4p_QJ8o",
        "outputId": "67c0d05c-9cf9-40e9-ed24-d6d0325384a8"
      },
      "source": [
        "len(res[0])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2c4fkW4QCC",
        "outputId": "bef57749-c973-4f16-f9fd-fef4ac2f59e5"
      },
      "source": [
        "experiment._env._terminal_state.get_amplitudes()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.70876626+0.01913417j, -0.38731851-0.04619398j,\n",
              "       -0.51588681-0.03314136j, -0.26780692+0.08001031j])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pss1hqFz3G_",
        "outputId": "cb708b77-5127-497e-d7f7-41595de71b87"
      },
      "source": [
        "experiment._env._terminal_state.fidelity_score(experiment._env._target_state)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.856652121674456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0Tz6BuBybfY",
        "outputId": "6efea5e7-16c1-46f6-f54d-3730158abb5e"
      },
      "source": [
        "len(experiment.run_inference()[0])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "380"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}